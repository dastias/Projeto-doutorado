{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMwB7MEykrwvYArmAAo2wUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dastias/Projeto-doutorado/blob/main/vibration_analysis_aero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqR8fb6N4y88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b42aae5-e1f4-40e1-b1b0-97f08d5f8728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets"
      ],
      "metadata": {
        "id": "YAtBPjt_tvKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf66decf-e7e9-4a62-aa8f-0fa8ad1b046d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vibration_analysis_aero.py - CÃ³digo completo corrigido\n",
        "\n",
        "import os, math\n",
        "import numpy as np\n",
        "import pywt\n",
        "from scipy import signal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "class SignalPreprocessor:\n",
        "    def __init__(self, window_size=1024, overlap=0.5, sampling_rate=12800):\n",
        "        self.window_size = window_size\n",
        "        self.overlap = overlap\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "    def remove_noise(self, data: np.ndarray) -> np.ndarray:\n",
        "        coeffs = pywt.wavedec(data, 'db4', level=4)\n",
        "        thr = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = pywt.threshold(coeffs[i], thr, mode='soft')\n",
        "        return pywt.waverec(coeffs, 'db4')\n",
        "\n",
        "    def generate_spectrogram(self, data: np.ndarray) -> np.ndarray:\n",
        "        nperseg = self.window_size\n",
        "        noverlap = int(nperseg * self.overlap)\n",
        "        if noverlap >= nperseg:\n",
        "            noverlap = nperseg - 1\n",
        "        _, _, Sxx = signal.spectrogram(\n",
        "            data,\n",
        "            fs=self.sampling_rate,\n",
        "            window='hann',\n",
        "            nperseg=nperseg,\n",
        "            noverlap=noverlap\n",
        "        )\n",
        "        Sxx = 10 * np.log10(Sxx + 1e-10)\n",
        "        return (Sxx - Sxx.min()) / (Sxx.max() - Sxx.min() + 1e-10)\n",
        "\n",
        "class UnsupervisedAeroDataset(Dataset):\n",
        "    def __init__(self, csv_paths, transform=None):\n",
        "        self.transform = transform\n",
        "        self.pre = SignalPreprocessor()\n",
        "        self.samples = []\n",
        "        self.raw_signals = []\n",
        "\n",
        "        for path in csv_paths:\n",
        "            data = np.loadtxt(path, delimiter=',')\n",
        "            for row in data:\n",
        "                x = row[2:]\n",
        "                den = self.pre.remove_noise(x)\n",
        "                spec = self.pre.generate_spectrogram(den)\n",
        "                tensor = torch.tensor(spec).float().unsqueeze(0)\n",
        "                if self.transform:\n",
        "                    tensor = self.transform(tensor)\n",
        "                self.samples.append(tensor)\n",
        "                self.raw_signals.append(x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.raw_signals[idx], self.pre.remove_noise(self.raw_signals[idx])\n",
        "\n",
        "class VibrationAnalysisCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        x = self.features(x).view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class LoRAAdapter(nn.Module):\n",
        "    def __init__(self, layer: nn.Linear, rank=4, scale=0.01):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "        self.layer.weight.requires_grad = False\n",
        "        if self.layer.bias is not None:\n",
        "            self.layer.bias.requires_grad = False\n",
        "\n",
        "        in_f, out_f = layer.in_features, layer.out_features\n",
        "        self.A = nn.Parameter(torch.zeros(in_f, rank))\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_f))\n",
        "        self.scale = scale\n",
        "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x) + (x @ self.A @ self.B) * self.scale\n",
        "\n",
        "class SimCLRModelLoRA(nn.Module):\n",
        "    def __init__(self, projection_dim=128, num_classes=2, lora_rank=4, lora_scale=0.01):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, projection_dim)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            LoRAAdapter(nn.Linear(128, 256), rank=lora_rank, scale=lora_scale),\n",
        "            nn.ReLU(), nn.Dropout(0.5),\n",
        "            LoRAAdapter(nn.Linear(256, num_classes), rank=lora_rank, scale=lora_scale)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        h = self.encoder(x).view(x.size(0), -1)\n",
        "        return self.projection(h)\n",
        "\n",
        "    def forward_classifier(self, x):\n",
        "        x = x.float()\n",
        "        h = self.encoder(x).view(x.size(0), -1)\n",
        "        return self.classifier(h)\n",
        "\n",
        "class ModelAgent:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scores = {}\n",
        "\n",
        "    def update_models(self, models: dict):\n",
        "        self.models = models\n",
        "        self.scores = {k: 1.0 for k in models}\n",
        "\n",
        "    def update_performance(self, name, score):\n",
        "        self.scores[name] = 0.9 * self.scores[name] + 0.1 * score\n",
        "\n",
        "    def select_best_model(self):\n",
        "        return max(self.scores, key=self.scores.get)\n",
        "\n",
        "class VibrationAnalyzer:\n",
        "    def __init__(self, config, agent):\n",
        "        self.config = config\n",
        "        self.agent = agent\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.pre = SignalPreprocessor()\n",
        "\n",
        "    def pretrain_simclr(self, loader):\n",
        "        model = self.agent.models['simclr_lora'].to(self.device)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=self.config['learning_rate'])\n",
        "        model.train()\n",
        "        for _ in range(self.config['simclr_epochs']):\n",
        "            for spec, _, _ in loader:\n",
        "                spec = spec.to(self.device).float()\n",
        "                opt.zero_grad()\n",
        "                loss = torch.norm(model(spec), dim=1).mean()\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    def extract_embeddings(self, loader):\n",
        "        model = self.agent.models['simclr_lora'].to(self.device).eval()\n",
        "        embs = []\n",
        "        with torch.no_grad():\n",
        "            for spec, _, _ in loader:\n",
        "                embs.append(model(spec.to(self.device).float()).cpu().numpy())\n",
        "        return np.vstack(embs)\n",
        "\n",
        "    def train_and_eval_fold(self, train_ds, val_ds, fold_idx):\n",
        "        # etapa temporÃ¡ria para extrair embeddings com modelo default\n",
        "        self.agent.update_models({\n",
        "            'simclr_lora': SimCLRModelLoRA(projection_dim=128, num_classes=2)\n",
        "        })\n",
        "        loader_all = DataLoader(train_ds, batch_size=self.config['batch_size'], shuffle=False)\n",
        "        embs = self.extract_embeddings(loader_all)\n",
        "\n",
        "        best_k, best_s = 2, -1\n",
        "        for k in range(2, 11):\n",
        "            km = KMeans(n_clusters=k, random_state=42).fit(embs)\n",
        "            s = silhouette_score(embs, km.labels_)\n",
        "            if s > best_s:\n",
        "                best_k, best_s = k, s\n",
        "\n",
        "        self.agent.update_models({\n",
        "            'cnn': VibrationAnalysisCNN(num_classes=best_k),\n",
        "            'simclr_lora': SimCLRModelLoRA(projection_dim=128, num_classes=best_k)\n",
        "        })\n",
        "\n",
        "        self.pretrain_simclr(DataLoader(train_ds, batch_size=self.config['batch_size'], shuffle=True))\n",
        "\n",
        "        km = KMeans(n_clusters=best_k, random_state=42).fit(embs)\n",
        "        pseudo = km.labels_\n",
        "\n",
        "        class PLDS(Dataset):\n",
        "            def __init__(self, base_ds, labels):\n",
        "                self.base, self.labels = base_ds, labels\n",
        "            def __len__(self): return len(self.base)\n",
        "            def __getitem__(self, i):\n",
        "                spec, raw, den = self.base[i]\n",
        "                return spec, self.labels[i], raw, den\n",
        "\n",
        "        sup_tr = PLDS(train_ds, pseudo)\n",
        "        sup_va = PLDS(val_ds, pseudo)\n",
        "\n",
        "        for name, model in self.agent.models.items():\n",
        "            model = model.to(self.device)\n",
        "            opt = torch.optim.Adam(model.parameters(), lr=self.config['learning_rate'])\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "            history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "            for ep in range(self.config['epochs']):\n",
        "                model.train(); tl = 0\n",
        "                for spec, label, _, _ in DataLoader(sup_tr, batch_size=self.config['batch_size'], shuffle=True):\n",
        "                    spec = spec.to(self.device).float()\n",
        "                    label = label.to(self.device).long()\n",
        "                    opt.zero_grad()\n",
        "                    out = model(spec) if name == 'cnn' else model.forward_classifier(spec)\n",
        "                    loss = loss_fn(out, label)\n",
        "                    loss.backward(); opt.step()\n",
        "                    tl += loss.item()\n",
        "                history['train_loss'].append(tl / len(sup_tr))\n",
        "\n",
        "                model.eval(); vl, corr = 0, 0\n",
        "                with torch.no_grad():\n",
        "                    for spec, label, _, _ in DataLoader(sup_va, batch_size=self.config['batch_size']):\n",
        "                        spec = spec.to(self.device).float()\n",
        "                        label = label.to(self.device).long()\n",
        "                        out = model(spec) if name == 'cnn' else model.forward_classifier(spec)\n",
        "                        vl += loss_fn(out, label).item()\n",
        "                        preds = out.argmax(dim=1)\n",
        "                        corr += (preds == label).sum().item()\n",
        "                history['val_loss'].append(vl / len(sup_va))\n",
        "                history['val_acc'].append(corr / len(sup_va))\n",
        "\n",
        "            # mÃ©tricas finais\n",
        "            all_preds, all_labels = [], []\n",
        "            with torch.no_grad():\n",
        "                for spec, label, _, _ in DataLoader(sup_va, batch_size=self.config['batch_size']):\n",
        "                    out = model(spec.to(self.device).float()) if name == 'cnn' else model.forward_classifier(spec.to(self.device).float())\n",
        "                    preds = out.argmax(dim=1).cpu().numpy()\n",
        "                    labs = label.cpu().numpy()\n",
        "                    all_preds.extend(preds)\n",
        "                    all_labels.extend(labs)\n",
        "\n",
        "            metrics = {\n",
        "                'accuracy': accuracy_score(all_labels, all_preds),\n",
        "                'precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
        "                'recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
        "                'f1': f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "            }\n",
        "            print(f\"{name} â acc {metrics['accuracy']:.3f}, prec {metrics['precision']:.3f}, rec {metrics['recall']:.3f}, f1 {metrics['f1']:.3f}\")\n",
        "            self.agent.update_performance(name, metrics['f1'])\n",
        "            pd.DataFrame([metrics]).to_csv(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_metrics.csv'), index=False)\n",
        "            dfh = pd.DataFrame(history)\n",
        "            dfh['epoch'] = np.arange(1, self.config['epochs'] + 1)\n",
        "            dfh.to_csv(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_history.csv'), index=False)\n",
        "            plt.figure()\n",
        "            plt.plot(dfh['epoch'], dfh['train_loss'], label='Train Loss')\n",
        "            plt.plot(dfh['epoch'], dfh['val_loss'], label='Val Loss')\n",
        "            plt.legend(); plt.title(f'Loss Curve â {name}')\n",
        "            plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_loss.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(dfh['epoch'], dfh['val_acc'], label='Val Accuracy')\n",
        "            plt.legend(); plt.title(f'Accuracy Curve â {name}')\n",
        "            plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_acc.png'))\n",
        "            plt.close()\n",
        "\n",
        "            cm = confusion_matrix(all_labels, all_preds)\n",
        "            plt.figure(figsize=(6,5))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "            plt.title(f'Matriz de ConfusÃ£o â {name}')\n",
        "            plt.xlabel('Predito')\n",
        "            plt.ylabel('Real')\n",
        "            plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_cm.png'))\n",
        "            plt.close()\n",
        "\n",
        "            torch.save(model.state_dict(), os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_model.pt'))\n",
        "\n",
        "    def cross_validate(self, dataset):\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        for i, (tr, va) in enumerate(kf.split(dataset)):\n",
        "            self.train_and_eval_fold(Subset(dataset, tr), Subset(dataset, va), i)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from torchvision import transforms\n",
        "\n",
        "    config = {\n",
        "        'learning_rate': 1e-3,\n",
        "        'simclr_epochs': 5,\n",
        "        'batch_size': 32,\n",
        "        'epochs': 10,\n",
        "        'output_dir': '/content/drive/My Drive/aero_results/'\n",
        "    }\n",
        "    os.makedirs(config['output_dir'], exist_ok=True)\n",
        "\n",
        "    tfm = transforms.Normalize([0.5], [0.5])\n",
        "    csvs = [\n",
        "        '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET06.csv',\n",
        "        '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET07.csv'\n",
        "    ]\n",
        "    ds = UnsupervisedAeroDataset(csvs, transform=tfm)\n",
        "\n",
        "    agent = ModelAgent()\n",
        "    analyzer = VibrationAnalyzer(config, agent)\n",
        "    analyzer.cross_validate(ds)\n"
      ],
      "metadata": {
        "id": "vM45-UPNxlds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447a6028-acbf-4c11-d9c8-28059b587a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnn â acc 0.562, prec 0.612, rec 0.562, f1 0.582\n",
            "simclr_lora â acc 0.578, prec 0.603, rec 0.578, f1 0.589\n",
            "cnn â acc 0.501, prec 0.545, rec 0.501, f1 0.514\n",
            "simclr_lora â acc 0.517, prec 0.549, rec 0.517, f1 0.528\n",
            "cnn â acc 0.547, prec 0.603, rec 0.547, f1 0.567\n",
            "simclr_lora â acc 0.557, prec 0.595, rec 0.557, f1 0.572\n",
            "cnn â acc 0.588, prec 0.639, rec 0.588, f1 0.608\n",
            "simclr_lora â acc 0.589, prec 0.641, rec 0.589, f1 0.609\n",
            "cnn â acc 0.542, prec 0.617, rec 0.542, f1 0.570\n",
            "simclr_lora â acc 0.559, prec 0.617, rec 0.559, f1 0.582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vibration_analysis_aero.py - CÃ³digo completo corrigido e com sugestÃµes implementadas\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pywt\n",
        "from scipy import signal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torchvision import transforms # Movido para o topo para melhor organizaÃ§Ã£o\n",
        "\n",
        "# Helper para garantir que os diretÃ³rios de output existem\n",
        "def ensure_dir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "class SignalPreprocessor:\n",
        "    def __init__(self, window_size=1024, overlap=0.5, sampling_rate=12800):\n",
        "        self.window_size = window_size\n",
        "        self.overlap = overlap\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "    def remove_noise(self, data: np.ndarray) -> np.ndarray:\n",
        "        coeffs = pywt.wavedec(data, 'db4', level=4)\n",
        "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "        thr = sigma * np.sqrt(2 * np.log(len(data))) if len(data) > 1 else 0 # Limiar de Donoho\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = pywt.threshold(coeffs[i], thr, mode='soft')\n",
        "        reconstructed_signal = pywt.waverec(coeffs, 'db4')\n",
        "        if len(reconstructed_signal) != len(data):\n",
        "            reconstructed_signal = reconstructed_signal[:len(data)]\n",
        "        return reconstructed_signal\n",
        "\n",
        "    def generate_spectrogram(self, data: np.ndarray) -> np.ndarray:\n",
        "        nperseg = self.window_size\n",
        "        noverlap = int(nperseg * self.overlap)\n",
        "        if noverlap >= nperseg:\n",
        "            noverlap = nperseg - 1\n",
        "\n",
        "        if len(data) < nperseg:\n",
        "            padding_length = nperseg - len(data)\n",
        "            data = np.pad(data, (0, padding_length), 'constant', constant_values=(0,0))\n",
        "\n",
        "        _, _, Sxx = signal.spectrogram(\n",
        "            data,\n",
        "            fs=self.sampling_rate,\n",
        "            window='hann',\n",
        "            nperseg=nperseg,\n",
        "            noverlap=noverlap\n",
        "        )\n",
        "        Sxx = 10 * np.log10(Sxx + 1e-10)\n",
        "        min_sxx = Sxx.min()\n",
        "        max_sxx = Sxx.max()\n",
        "        if max_sxx - min_sxx < 1e-10:\n",
        "            return np.zeros_like(Sxx)\n",
        "        return (Sxx - min_sxx) / (max_sxx - min_sxx + 1e-10)\n",
        "\n",
        "class UnsupervisedAeroDataset(Dataset):\n",
        "    def __init__(self, csv_paths, signal_preprocessor: SignalPreprocessor, transform=None):\n",
        "        self.transform = transform\n",
        "        self.pre = signal_preprocessor\n",
        "        self.samples = []\n",
        "\n",
        "        print(f\"Carregando e prÃ©-processando dados de: {csv_paths}\")\n",
        "        for path_idx, path in enumerate(csv_paths):\n",
        "            try:\n",
        "                raw_data_file = pd.read_csv(path, header=None, low_memory=False)\n",
        "                for row_idx, row_series in raw_data_file.iterrows():\n",
        "                    try:\n",
        "                        signal_data = pd.to_numeric(row_series.iloc[2:], errors='coerce').to_numpy()\n",
        "                        signal_data = signal_data[~np.isnan(signal_data)]\n",
        "\n",
        "                        if len(signal_data) < self.pre.window_size :\n",
        "                            continue\n",
        "\n",
        "                        denoised_signal = self.pre.remove_noise(signal_data)\n",
        "\n",
        "                        if len(denoised_signal) < self.pre.window_size:\n",
        "                             continue\n",
        "\n",
        "                        spectrogram = self.pre.generate_spectrogram(denoised_signal)\n",
        "                        tensor = torch.tensor(spectrogram, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "                        if self.transform:\n",
        "                            tensor = self.transform(tensor)\n",
        "\n",
        "                        self.samples.append(tensor)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Erro processando linha {row_idx+1} do arquivo {path}: {e}. Pulando linha.\")\n",
        "                        continue\n",
        "            except Exception as e:\n",
        "                print(f\"Erro lendo ou processando arquivo CSV {path}: {e}. Pulando arquivo.\")\n",
        "                continue\n",
        "        print(f\"Total de {len(self.samples)} amostras carregadas.\")\n",
        "        if not self.samples:\n",
        "            raise ValueError(\"Nenhuma amostra foi carregada. Verifique os arquivos CSV e o prÃ©-processamento.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "class PLDS(Dataset): # Pseudo-Label Dataset\n",
        "    def __init__(self, base_subset: Subset, labels: np.ndarray):\n",
        "        self.base_subset = base_subset\n",
        "        self.labels = labels\n",
        "        assert len(self.base_subset) == len(self.labels), \\\n",
        "            f\"Dataset (len: {len(self.base_subset)}) e labels (len: {len(self.labels)}) devem ter o mesmo tamanho.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spectrogram_tensor = self.base_subset[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Retorna apenas os dados relevantes\n",
        "        return spectrogram_tensor, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "class VibrationAnalysisCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class LoRAAdapter(nn.Module):\n",
        "    def __init__(self, layer: nn.Linear, rank=4, scale=0.01):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "        self.layer.weight.requires_grad = False\n",
        "        if self.layer.bias is not None:\n",
        "            self.layer.bias.requires_grad = False\n",
        "\n",
        "        in_f, out_f = layer.in_features, layer.out_features\n",
        "        self.A = nn.Parameter(torch.Tensor(in_f, rank))\n",
        "        self.B = nn.Parameter(torch.Tensor(rank, out_f))\n",
        "        self.scale = scale\n",
        "\n",
        "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x) + (x @ self.A @ self.B) * self.scale\n",
        "\n",
        "\n",
        "class SimCLRModelLoRA(nn.Module):\n",
        "    def __init__(self, projection_dim=128, num_classes=2, lora_rank=4, lora_scale=0.01):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, projection_dim)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            LoRAAdapter(nn.Linear(128, 256), rank=lora_rank, scale=lora_scale),\n",
        "            nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            LoRAAdapter(nn.Linear(256, num_classes), rank=lora_rank, scale=lora_scale)\n",
        "        )\n",
        "\n",
        "    def forward_encoder_projection(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        return self.projection(h)\n",
        "\n",
        "    def forward_classifier(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        return self.classifier(h)\n",
        "\n",
        "\n",
        "class ModelAgent:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scores = {}\n",
        "\n",
        "    def update_models(self, models: dict):\n",
        "        self.models = models\n",
        "        self.scores = {name: 0.0 for name in models}\n",
        "\n",
        "    def update_performance(self, model_name, score):\n",
        "        self.scores[model_name] = score\n",
        "\n",
        "    def get_best_model_name(self):\n",
        "        if not self.scores:\n",
        "            return None\n",
        "        return max(self.scores, key=self.scores.get)\n",
        "\n",
        "\n",
        "class VibrationAnalyzer:\n",
        "    def __init__(self, config, agent: ModelAgent, signal_preprocessor: SignalPreprocessor):\n",
        "        self.config = config\n",
        "        self.agent = agent\n",
        "        self.signal_preprocessor = signal_preprocessor\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Usando dispositivo: {self.device}\")\n",
        "        ensure_dir(self.config['output_dir'])\n",
        "\n",
        "    def _train_encoder_module(self, model_to_train: SimCLRModelLoRA, loader: DataLoader, epochs: int, learning_rate: float):\n",
        "        print(f\"Iniciando prÃ©-treinamento do encoder por {epochs} Ã©pocas...\")\n",
        "        model_to_train.to(self.device)\n",
        "        optimizer = torch.optim.Adam(list(model_to_train.encoder.parameters()) + list(model_to_train.projection.parameters()), lr=learning_rate)\n",
        "\n",
        "        model_to_train.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for batch_idx, spectrograms in enumerate(loader): # CORRIGIDO AQUI\n",
        "                spectrograms = spectrograms.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                projections = model_to_train.forward_encoder_projection(spectrograms)\n",
        "                loss = torch.norm(projections, p=2, dim=1).mean()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_loss = total_loss / len(loader) if len(loader) > 0 else 0\n",
        "            print(f\"Encoder Pretrain Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "        print(\"PrÃ©-treinamento do encoder concluÃ­do.\")\n",
        "\n",
        "    def _extract_embeddings_from_module(self, model_extractor: SimCLRModelLoRA, loader: DataLoader) -> np.ndarray:\n",
        "        model_extractor.to(self.device)\n",
        "        model_extractor.eval()\n",
        "        all_embeddings = []\n",
        "        with torch.no_grad():\n",
        "            for spectrograms in loader: # CORRIGIDO AQUI\n",
        "                spectrograms = spectrograms.to(self.device)\n",
        "                embeddings = model_extractor.forward_encoder_projection(spectrograms)\n",
        "                all_embeddings.append(embeddings.cpu().numpy())\n",
        "        if not all_embeddings: # Lidar com caso de lista vazia\n",
        "            return np.array([])\n",
        "        return np.vstack(all_embeddings)\n",
        "\n",
        "    def train_and_eval_fold(self, train_subset: Subset, val_subset: Subset, fold_idx: int):\n",
        "        print(f\"\\n--- Fold {fold_idx + 1} ---\")\n",
        "\n",
        "        # ETAPA 1: PrÃ©-treinamento do Encoder para ExtraÃ§Ã£o de Embeddings\n",
        "        print(f\"Fold {fold_idx + 1}: (Etapa 1) PrÃ©-treinando encoder para K-Means...\")\n",
        "        encoder_trainer_model = SimCLRModelLoRA(\n",
        "            projection_dim=self.config['projection_dim_simclr'],\n",
        "            num_classes=2\n",
        "        ).to(self.device)\n",
        "\n",
        "        pretrain_loader = DataLoader(train_subset, batch_size=self.config['batch_size'], shuffle=True,\n",
        "                                     num_workers=self.config.get('num_workers', 0), pin_memory=True) # num_workers=0 para debug no Colab se necessÃ¡rio\n",
        "\n",
        "        if len(pretrain_loader) > 0:\n",
        "            self._train_encoder_module(encoder_trainer_model, pretrain_loader,\n",
        "                                    self.config['simclr_epochs_for_kmeans'], self.config['learning_rate_simclr_kmeans'])\n",
        "        else:\n",
        "            print(f\"Fold {fold_idx + 1}: Pretrain_loader vazio. Pulando prÃ©-treinamento do encoder.\")\n",
        "\n",
        "\n",
        "        # ETAPA 2: ExtraÃ§Ã£o de Embeddings com o Encoder PrÃ©-treinado\n",
        "        print(f\"Fold {fold_idx + 1}: (Etapa 2) Extraindo embeddings...\")\n",
        "        emb_loader_train = DataLoader(train_subset, batch_size=self.config['batch_size'], shuffle=False,\n",
        "                                      num_workers=self.config.get('num_workers', 0), pin_memory=True)\n",
        "        emb_loader_val = DataLoader(val_subset, batch_size=self.config['batch_size'], shuffle=False,\n",
        "                                    num_workers=self.config.get('num_workers', 0), pin_memory=True)\n",
        "\n",
        "        train_embeddings = self._extract_embeddings_from_module(encoder_trainer_model, emb_loader_train)\n",
        "        val_embeddings = self._extract_embeddings_from_module(encoder_trainer_model, emb_loader_val)\n",
        "\n",
        "        if len(train_embeddings) == 0:\n",
        "            print(f\"Fold {fold_idx + 1}: Nenhum embedding de treino extraÃ­do. Pulando fold.\")\n",
        "            return\n",
        "\n",
        "        # ETAPA 3: K-Means para NÃºmero de Classes e Pseudo-RÃ³tulos\n",
        "        print(f\"Fold {fold_idx + 1}: (Etapa 3) Executando K-Means...\")\n",
        "        best_k, best_silhouette_score = 2, -1.0\n",
        "\n",
        "        max_possible_k = min(self.config['max_k_means_clusters'], len(train_embeddings))\n",
        "\n",
        "        if max_possible_k < 2:\n",
        "            print(f\"Fold {fold_idx + 1}: NÃºmero insuficiente de amostras de treino ({len(train_embeddings)}) para K-Means com k >= 2. Usando k=2 por padrÃ£o.\")\n",
        "            best_k = 2\n",
        "            if len(train_embeddings) < 2 :\n",
        "                 print(f\"Fold {fold_idx + 1}: Menos de 2 amostras de treino. ImpossÃ­vel prosseguir com classificaÃ§Ã£o. Pulando fold.\")\n",
        "                 return\n",
        "        else:\n",
        "            for k_try in range(2, max_possible_k + 1):\n",
        "                try:\n",
        "                    kmeans = KMeans(n_clusters=k_try, random_state=self.config['random_seed'], n_init='auto').fit(train_embeddings)\n",
        "                    if len(np.unique(kmeans.labels_)) > 1:\n",
        "                        score = silhouette_score(train_embeddings, kmeans.labels_)\n",
        "                        if score > best_silhouette_score:\n",
        "                            best_silhouette_score = score\n",
        "                            best_k = k_try\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro durante K-Means ou Silhouette para k={k_try}: {e}\")\n",
        "                    continue\n",
        "        print(f\"Fold {fold_idx + 1}: NÃºmero de classes estimado (best_k) = {best_k} (Silhouette: {best_silhouette_score:.3f})\")\n",
        "\n",
        "        final_kmeans = KMeans(n_clusters=best_k, random_state=self.config['random_seed'], n_init='auto').fit(train_embeddings)\n",
        "        train_pseudo_labels = final_kmeans.labels_\n",
        "\n",
        "        val_pseudo_labels = np.array([])\n",
        "        if len(val_embeddings) > 0:\n",
        "            try:\n",
        "                val_pseudo_labels = final_kmeans.predict(val_embeddings)\n",
        "            except Exception as e: # Kmeans pode nÃ£o conseguir prever se val_embeddings for muito diferente\n",
        "                print(f\"Erro ao prever pseudo-rÃ³tulos de validaÃ§Ã£o: {e}. ValidaÃ§Ã£o serÃ¡ limitada.\")\n",
        "        else:\n",
        "            print(f\"Fold {fold_idx + 1}: Nenhum embedding de validaÃ§Ã£o. ValidaÃ§Ã£o serÃ¡ limitada.\")\n",
        "\n",
        "\n",
        "        # ETAPA 4: Instanciar e Treinar Modelos Finais com Pseudo-RÃ³tulos\n",
        "        print(f\"Fold {fold_idx + 1}: (Etapa 4) Preparando modelos finais...\")\n",
        "        cnn_classifier = VibrationAnalysisCNN(num_classes=best_k).to(self.device)\n",
        "\n",
        "        simclr_lora_classifier = SimCLRModelLoRA(\n",
        "            projection_dim=self.config['projection_dim_simclr'],\n",
        "            num_classes=best_k,\n",
        "            lora_rank=self.config['lora_rank'],\n",
        "            lora_scale=self.config['lora_scale']\n",
        "        ).to(self.device)\n",
        "\n",
        "        if hasattr(encoder_trainer_model, 'encoder'): # Checar se o modelo de treino do encoder existe\n",
        "            simclr_lora_classifier.encoder.load_state_dict(encoder_trainer_model.encoder.state_dict())\n",
        "            print(f\"Fold {fold_idx + 1}: Pesos do encoder prÃ©-treinado carregados no SimCLRModelLoRA final.\")\n",
        "\n",
        "            if self.config['freeze_encoder_after_load']:\n",
        "                for param in simclr_lora_classifier.encoder.parameters():\n",
        "                    param.requires_grad = False\n",
        "                print(f\"Fold {fold_idx + 1}: Encoder do SimCLRModelLoRA final congelado.\")\n",
        "\n",
        "        self.agent.update_models({\n",
        "            'VibrationCNN': cnn_classifier,\n",
        "            'SimCLR_LoRA_Classifier': simclr_lora_classifier\n",
        "        })\n",
        "\n",
        "        sup_train_dataset = PLDS(train_subset, train_pseudo_labels)\n",
        "\n",
        "        sup_val_dataset = None\n",
        "        if len(val_pseudo_labels) > 0 and len(val_subset) == len(val_pseudo_labels):\n",
        "            sup_val_dataset = PLDS(val_subset, val_pseudo_labels)\n",
        "        else:\n",
        "            print(f\"Fold {fold_idx + 1}: Conjunto de validaÃ§Ã£o ou pseudo-rÃ³tulos de validaÃ§Ã£o incompatÃ­veis/vazios. ValidaÃ§Ã£o serÃ¡ limitada.\")\n",
        "\n",
        "\n",
        "        # Loop de Treinamento Supervisionado\n",
        "        for model_name, model_instance in self.agent.models.items():\n",
        "            print(f\"\\nFold {fold_idx + 1}: Treinando modelo '{model_name}'...\")\n",
        "            model_instance.to(self.device)\n",
        "\n",
        "            trainable_params = filter(lambda p: p.requires_grad, model_instance.parameters())\n",
        "            optimizer = torch.optim.Adam(trainable_params, lr=self.config['learning_rate_classifier'])\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "            train_loader_sup = DataLoader(sup_train_dataset, batch_size=self.config['batch_size'], shuffle=True,\n",
        "                                          num_workers=self.config.get('num_workers', 0), pin_memory=True)\n",
        "\n",
        "            val_loader_sup = None\n",
        "            if sup_val_dataset and len(sup_val_dataset) > 0:\n",
        "                 val_loader_sup = DataLoader(sup_val_dataset, batch_size=self.config['batch_size'], shuffle=False,\n",
        "                                        num_workers=self.config.get('num_workers', 0), pin_memory=True)\n",
        "\n",
        "            for epoch in range(self.config['epochs_classifier']):\n",
        "                model_instance.train()\n",
        "                epoch_train_loss = 0\n",
        "                if len(train_loader_sup) == 0:\n",
        "                    print(f\"Epoch {epoch+1}/{self.config['epochs_classifier']} - {model_name} - Train loader vazio. Pulando treino.\")\n",
        "                    history['train_loss'].append(0)\n",
        "                    history['epoch'].append(epoch + 1)\n",
        "                    history['val_loss'].append(0)\n",
        "                    history['val_acc'].append(0)\n",
        "                    continue # Pula para a prÃ³xima Ã©poca\n",
        "\n",
        "                for batch_idx, (specs, labels) in enumerate(train_loader_sup): # CORRIGIDO AQUI\n",
        "                    specs, labels = specs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    if model_name == 'VibrationCNN':\n",
        "                        outputs = model_instance(specs)\n",
        "                    else:\n",
        "                        outputs = model_instance.forward_classifier(specs)\n",
        "\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_train_loss += loss.item()\n",
        "\n",
        "                avg_epoch_train_loss = epoch_train_loss / len(train_loader_sup) if len(train_loader_sup) > 0 else 0\n",
        "                history['train_loss'].append(avg_epoch_train_loss)\n",
        "                history['epoch'].append(epoch + 1)\n",
        "\n",
        "                epoch_val_loss = 0\n",
        "                correct_val = 0\n",
        "                total_val = 0\n",
        "                all_preds_val, all_labels_val = [], []\n",
        "\n",
        "                if val_loader_sup and len(val_loader_sup) > 0:\n",
        "                    model_instance.eval()\n",
        "                    with torch.no_grad():\n",
        "                        for specs, labels in val_loader_sup: # CORRIGIDO AQUI\n",
        "                            specs, labels = specs.to(self.device), labels.to(self.device)\n",
        "                            if model_name == 'VibrationCNN':\n",
        "                                outputs = model_instance(specs)\n",
        "                            else:\n",
        "                                outputs = model_instance.forward_classifier(specs)\n",
        "\n",
        "                            loss_val = criterion(outputs, labels) # Renomear variÃ¡vel de loss\n",
        "                            epoch_val_loss += loss_val.item()\n",
        "\n",
        "                            _, predicted = torch.max(outputs.data, 1)\n",
        "                            total_val += labels.size(0)\n",
        "                            correct_val += (predicted == labels).sum().item()\n",
        "                            all_preds_val.extend(predicted.cpu().numpy())\n",
        "                            all_labels_val.extend(labels.cpu().numpy())\n",
        "\n",
        "                    avg_epoch_val_loss = epoch_val_loss / len(val_loader_sup) if len(val_loader_sup) > 0 else 0\n",
        "                    val_accuracy = correct_val / total_val if total_val > 0 else 0\n",
        "                    history['val_loss'].append(avg_epoch_val_loss)\n",
        "                    history['val_acc'].append(val_accuracy)\n",
        "                    print(f\"Epoch {epoch+1}/{self.config['epochs_classifier']} - {model_name} - Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_epoch_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "                else:\n",
        "                    history['val_loss'].append(0)\n",
        "                    history['val_acc'].append(0)\n",
        "                    print(f\"Epoch {epoch+1}/{self.config['epochs_classifier']} - {model_name} - Train Loss: {avg_epoch_train_loss:.4f} (ValidaÃ§Ã£o pulada ou loader vazio)\")\n",
        "\n",
        "            if val_loader_sup and total_val > 0 :\n",
        "                metrics = {\n",
        "                    'accuracy': accuracy_score(all_labels_val, all_preds_val),\n",
        "                    'precision': precision_score(all_labels_val, all_preds_val, average='weighted', zero_division=0),\n",
        "                    'recall': recall_score(all_labels_val, all_preds_val, average='weighted', zero_division=0),\n",
        "                    'f1': f1_score(all_labels_val, all_preds_val, average='weighted', zero_division=0)\n",
        "                }\n",
        "                print(f\"Fold {fold_idx + 1} - Modelo '{model_name}' - MÃ©tricas Finais (Val): \"\n",
        "                      f\"Acc: {metrics['accuracy']:.3f}, Prec: {metrics['precision']:.3f}, \"\n",
        "                      f\"Rec: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}\")\n",
        "                self.agent.update_performance(model_name, metrics['f1'])\n",
        "\n",
        "                metrics_df = pd.DataFrame([metrics])\n",
        "                metrics_df.to_csv(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_metrics.csv'), index=False)\n",
        "\n",
        "                cm = confusion_matrix(all_labels_val, all_preds_val, labels=list(range(best_k))) # Adicionar labels para garantir o tamanho da matriz\n",
        "                plt.figure(figsize=(max(6, best_k), max(5, best_k-1)))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(best_k), yticklabels=range(best_k))\n",
        "                plt.title(f'Matriz de ConfusÃ£o - Fold {fold_idx+1} - {model_name}')\n",
        "                plt.xlabel('Predito'); plt.ylabel('Real')\n",
        "                plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_cm.png'))\n",
        "                plt.close()\n",
        "            else:\n",
        "                print(f\"Fold {fold_idx + 1} - Modelo '{model_name}' - ValidaÃ§Ã£o pulada ou sem dados vÃ¡lidos para mÃ©tricas.\")\n",
        "                self.agent.update_performance(model_name, 0.0)\n",
        "\n",
        "            history_df = pd.DataFrame(history)\n",
        "            history_df.to_csv(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_history.csv'), index=False)\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(history_df['epoch'], history_df['train_loss'], label='Train Loss')\n",
        "            if val_loader_sup and len(val_loader_sup) > 0 : plt.plot(history_df['epoch'], history_df['val_loss'], label='Val Loss')\n",
        "            plt.legend(); plt.title(f'Curva de Loss - Fold {fold_idx+1} - {model_name}')\n",
        "            plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "            plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_loss_curve.png'))\n",
        "            plt.close()\n",
        "\n",
        "            if val_loader_sup and len(val_loader_sup) > 0:\n",
        "                plt.figure()\n",
        "                plt.plot(history_df['epoch'], history_df['val_acc'], label='Val Accuracy')\n",
        "                plt.legend(); plt.title(f'Curva de AcurÃ¡cia (Val) - Fold {fold_idx+1} - {model_name}')\n",
        "                plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
        "                plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_acc_curve.png'))\n",
        "                plt.close()\n",
        "\n",
        "            torch.save(model_instance.state_dict(), os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_model.pt'))\n",
        "            print(f\"Modelo '{model_name}' salvo para o Fold {fold_idx+1}.\")\n",
        "\n",
        "\n",
        "    def cross_validate(self, full_dataset: UnsupervisedAeroDataset):\n",
        "        if len(full_dataset) < self.config.get('n_folds', 1) : # n_folds deve ser pelo menos 1\n",
        "            print(f\"Erro: NÃºmero de amostras ({len(full_dataset)}) Ã© menor que o nÃºmero mÃ­nimo de folds requerido.\")\n",
        "            print(\"Reduza n_folds ou forneÃ§a mais dados.\")\n",
        "            return\n",
        "\n",
        "        kf = KFold(n_splits=self.config['n_folds'], shuffle=True, random_state=self.config['random_seed'])\n",
        "\n",
        "        for fold_index, (train_indices, val_indices) in enumerate(kf.split(range(len(full_dataset)))): # Usar range(len()) para kf.split\n",
        "            if len(train_indices) == 0 or len(val_indices) == 0:\n",
        "                print(f\"Fold {fold_index + 1} tem 0 amostras de treino ou validaÃ§Ã£o. Pulando este fold.\")\n",
        "                continue\n",
        "            train_subset = Subset(full_dataset, train_indices)\n",
        "            val_subset = Subset(full_dataset, val_indices)\n",
        "            self.train_and_eval_fold(train_subset, val_subset, fold_index)\n",
        "\n",
        "        print(\"\\n--- ValidaÃ§Ã£o Cruzada ConcluÃ­da ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = {\n",
        "        'csv_paths': [\n",
        "          '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET06.csv',\n",
        "          '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET07.csv'\n",
        "        ],\n",
        "        'window_size': 1024, 'overlap': 0.5, 'sampling_rate': 12800,\n",
        "        'output_dir': '/content/drive/My Drive/aero_results/',\n",
        "        'random_seed': 42,\n",
        "        'num_workers': 0, # Para Colab, 0 ou 2. Comece com 0 se houver problemas com workers.\n",
        "\n",
        "        'projection_dim_simclr': 128,\n",
        "        'simclr_epochs_for_kmeans': 5,\n",
        "        'learning_rate_simclr_kmeans': 1e-4,\n",
        "        'max_k_means_clusters': 10,\n",
        "\n",
        "        'learning_rate_classifier': 1e-4,\n",
        "        'epochs_classifier': 30,\n",
        "        'batch_size': 32,\n",
        "\n",
        "        'lora_rank': 8, 'lora_scale': 0.01,\n",
        "        'freeze_encoder_after_load': False,\n",
        "\n",
        "        'n_folds': 5\n",
        "    }\n",
        "\n",
        "    np.random.seed(config['random_seed'])\n",
        "    torch.manual_seed(config['random_seed'])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(config['random_seed'])\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    if not config['csv_paths']:\n",
        "        print(\"Nenhum caminho CSV fornecido. Criando dummy CSVs para teste.\")\n",
        "        ensure_dir('dummy_data_final')\n",
        "        for i in range(2):\n",
        "            dummy_signal_length = config['window_size'] * 2\n",
        "            data = np.random.rand(100, 2 + dummy_signal_length)\n",
        "            pd.DataFrame(data).to_csv(f'dummy_data_final/dummy_dataset_{i+1}.csv', header=False, index=False)\n",
        "            config['csv_paths'].append(f'dummy_data_final/dummy_dataset_{i+1}.csv')\n",
        "\n",
        "    signal_preprocessor = SignalPreprocessor(\n",
        "        window_size=config['window_size'],\n",
        "        overlap=config['overlap'],\n",
        "        sampling_rate=config['sampling_rate']\n",
        "    )\n",
        "\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        full_aero_dataset = UnsupervisedAeroDataset(\n",
        "            csv_paths=config['csv_paths'],\n",
        "            signal_preprocessor=signal_preprocessor,\n",
        "            transform=data_transform\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        print(f\"Erro ao inicializar o dataset: {e}\")\n",
        "        print(\"Encerrando o script.\")\n",
        "        exit()\n",
        "\n",
        "    if len(full_aero_dataset) == 0:\n",
        "        print(\"Dataset estÃ¡ vazio apÃ³s o processamento. Verifique os dados de entrada e os logs.\")\n",
        "        print(\"Encerrando o script.\")\n",
        "        exit()\n",
        "\n",
        "    print(f\"Dataset carregado com {len(full_aero_dataset)} amostras.\")\n",
        "\n",
        "    model_agent = ModelAgent()\n",
        "    vibration_analyzer = VibrationAnalyzer(config, model_agent, signal_preprocessor)\n",
        "\n",
        "    vibration_analyzer.cross_validate(full_aero_dataset)\n",
        "\n",
        "    print(\"\\nAnÃ¡lise concluÃ­da.\")\n",
        "    if model_agent.models:\n",
        "        print(\"Scores F1 finais (do Ãºltimo fold processado com sucesso):\")\n",
        "        for name, score in model_agent.scores.items():\n",
        "            print(f\"  {name}: {score:.4f}\")\n",
        "        best_model_overall = model_agent.get_best_model_name()\n",
        "        if best_model_overall:\n",
        "            print(f\"Melhor modelo no Ãºltimo fold (baseado em F1): {best_model_overall} \"\n",
        "                  f\"(F1: {model_agent.scores[best_model_overall]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLuHxw-k8YpL",
        "outputId": "a010f01f-4da3-4fb9-c5cb-fe79d35750cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando e prÃ©-processando dados de: ['/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET06.csv', '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET07.csv']\n",
            "Total de 5929 amostras carregadas.\n",
            "Dataset carregado com 5929 amostras.\n",
            "Usando dispositivo: cuda\n",
            "\n",
            "--- Fold 1 ---\n",
            "Fold 1: (Etapa 1) PrÃ©-treinando encoder para K-Means...\n",
            "Iniciando prÃ©-treinamento do encoder por 5 Ã©pocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0798\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0040\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0029\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0028\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0025\n",
            "PrÃ©-treinamento do encoder concluÃ­do.\n",
            "Fold 1: (Etapa 2) Extraindo embeddings...\n",
            "Fold 1: (Etapa 3) Executando K-Means...\n",
            "Fold 1: NÃºmero de classes estimado (best_k) = 2 (Silhouette: 0.232)\n",
            "Fold 1: (Etapa 4) Preparando modelos finais...\n",
            "Fold 1: Pesos do encoder prÃ©-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 1: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6913, Val Loss: 0.6906, Val Acc: 0.5253\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6892, Val Loss: 0.6896, Val Acc: 0.5253\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6898, Val Loss: 0.6888, Val Acc: 0.5253\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6886, Val Loss: 0.6881, Val Acc: 0.5337\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6884, Val Loss: 0.6881, Val Acc: 0.5253\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6888, Val Loss: 0.6872, Val Acc: 0.5388\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6882, Val Loss: 0.6862, Val Acc: 0.5388\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6864, Val Loss: 0.6858, Val Acc: 0.5430\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6853, Val Loss: 0.6864, Val Acc: 0.5253\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6838, Val Loss: 0.6802, Val Acc: 0.5540\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.6781, Val Loss: 0.6740, Val Acc: 0.5641\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.6714, Val Loss: 0.6555, Val Acc: 0.6231\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.6494, Val Loss: 0.6341, Val Acc: 0.6062\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.6279, Val Loss: 0.6173, Val Acc: 0.6703\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.5870, Val Loss: 0.6090, Val Acc: 0.6315\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.5641, Val Loss: 0.5358, Val Acc: 0.7782\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.5332, Val Loss: 0.5250, Val Acc: 0.7437\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.5016, Val Loss: 0.5738, Val Acc: 0.6771\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.5284, Val Loss: 0.4848, Val Acc: 0.7884\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.4980, Val Loss: 0.4808, Val Acc: 0.7884\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.4722, Val Loss: 0.4981, Val Acc: 0.7639\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.4845, Val Loss: 0.5872, Val Acc: 0.7015\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.4632, Val Loss: 0.4778, Val Acc: 0.7850\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.4677, Val Loss: 0.4601, Val Acc: 0.7909\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4471, Val Loss: 0.4485, Val Acc: 0.8010\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.4442, Val Loss: 0.4481, Val Acc: 0.7968\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4648, Val Loss: 0.4438, Val Acc: 0.7993\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.4421, Val Loss: 0.5335, Val Acc: 0.7496\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.4448, Val Loss: 0.4676, Val Acc: 0.7892\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.4465, Val Loss: 0.4452, Val Acc: 0.8002\n",
            "Fold 1 - Modelo 'VibrationCNN' - MÃ©tricas Finais (Val): Acc: 0.800, Prec: 0.803, Rec: 0.800, F1: 0.799\n",
            "Modelo 'VibrationCNN' salvo para o Fold 1.\n",
            "\n",
            "Fold 1: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6899, Val Loss: 0.6919, Val Acc: 0.5253\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6901, Val Loss: 0.6896, Val Acc: 0.5253\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6889, Val Loss: 0.6891, Val Acc: 0.5253\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6890, Val Loss: 0.6893, Val Acc: 0.5253\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6892, Val Loss: 0.6881, Val Acc: 0.5253\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6889, Val Loss: 0.6878, Val Acc: 0.5270\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6892, Val Loss: 0.6878, Val Acc: 0.5270\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6880, Val Loss: 0.6873, Val Acc: 0.5312\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6882, Val Loss: 0.6868, Val Acc: 0.5363\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6878, Val Loss: 0.6868, Val Acc: 0.5582\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6880, Val Loss: 0.6855, Val Acc: 0.5388\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6861, Val Loss: 0.6842, Val Acc: 0.5388\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6858, Val Loss: 0.6872, Val Acc: 0.5253\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6830, Val Loss: 0.6827, Val Acc: 0.5295\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6831, Val Loss: 0.6793, Val Acc: 0.5413\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6807, Val Loss: 0.6756, Val Acc: 0.6223\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6775, Val Loss: 0.6710, Val Acc: 0.5691\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6729, Val Loss: 0.6721, Val Acc: 0.5278\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6697, Val Loss: 0.6622, Val Acc: 0.6020\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6651, Val Loss: 0.6592, Val Acc: 0.5632\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6575, Val Loss: 0.6478, Val Acc: 0.6610\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6516, Val Loss: 0.6456, Val Acc: 0.7201\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6461, Val Loss: 0.6362, Val Acc: 0.6383\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6387, Val Loss: 0.6225, Val Acc: 0.7091\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6306, Val Loss: 0.6155, Val Acc: 0.7352\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6227, Val Loss: 0.6199, Val Acc: 0.6324\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6153, Val Loss: 0.5974, Val Acc: 0.7403\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6061, Val Loss: 0.5908, Val Acc: 0.7234\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6054, Val Loss: 0.5807, Val Acc: 0.7462\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.5893, Val Loss: 0.5709, Val Acc: 0.7656\n",
            "Fold 1 - Modelo 'SimCLR_LoRA_Classifier' - MÃ©tricas Finais (Val): Acc: 0.766, Prec: 0.766, Rec: 0.766, F1: 0.766\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 1.\n",
            "\n",
            "--- Fold 2 ---\n",
            "Fold 2: (Etapa 1) PrÃ©-treinando encoder para K-Means...\n",
            "Iniciando prÃ©-treinamento do encoder por 5 Ã©pocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0772\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0054\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0033\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0027\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0025\n",
            "PrÃ©-treinamento do encoder concluÃ­do.\n",
            "Fold 2: (Etapa 2) Extraindo embeddings...\n",
            "Fold 2: (Etapa 3) Executando K-Means...\n",
            "Fold 2: NÃºmero de classes estimado (best_k) = 2 (Silhouette: 0.208)\n",
            "Fold 2: (Etapa 4) Preparando modelos finais...\n",
            "Fold 2: Pesos do encoder prÃ©-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 2: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6766, Val Loss: 0.6756, Val Acc: 0.5843\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6747, Val Loss: 0.6738, Val Acc: 0.5843\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6734, Val Loss: 0.6746, Val Acc: 0.5843\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6744, Val Loss: 0.6727, Val Acc: 0.5843\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6730, Val Loss: 0.6736, Val Acc: 0.5843\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6705, Val Loss: 0.6711, Val Acc: 0.5843\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6691, Val Loss: 0.6675, Val Acc: 0.5843\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6631, Val Loss: 0.6614, Val Acc: 0.5843\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6486, Val Loss: 0.6363, Val Acc: 0.6113\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6286, Val Loss: 0.6137, Val Acc: 0.6113\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.5968, Val Loss: 0.5830, Val Acc: 0.7243\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.5626, Val Loss: 0.5517, Val Acc: 0.7243\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.5367, Val Loss: 0.5291, Val Acc: 0.7639\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.5159, Val Loss: 0.4952, Val Acc: 0.7732\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.5021, Val Loss: 0.4811, Val Acc: 0.7901\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.4722, Val Loss: 0.5075, Val Acc: 0.7589\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.4640, Val Loss: 0.4817, Val Acc: 0.7774\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.4489, Val Loss: 0.4744, Val Acc: 0.7757\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.4327, Val Loss: 0.4259, Val Acc: 0.8052\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.4292, Val Loss: 0.4152, Val Acc: 0.8212\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.4358, Val Loss: 0.4549, Val Acc: 0.7884\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.4181, Val Loss: 0.4899, Val Acc: 0.7664\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.4097, Val Loss: 0.4543, Val Acc: 0.7901\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.3973, Val Loss: 0.4888, Val Acc: 0.7614\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4066, Val Loss: 0.3885, Val Acc: 0.8297\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.3955, Val Loss: 0.3998, Val Acc: 0.8145\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4002, Val Loss: 0.4224, Val Acc: 0.8061\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.3864, Val Loss: 0.4099, Val Acc: 0.8069\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.3807, Val Loss: 0.5080, Val Acc: 0.7589\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.3987, Val Loss: 0.3638, Val Acc: 0.8474\n",
            "Fold 2 - Modelo 'VibrationCNN' - MÃ©tricas Finais (Val): Acc: 0.847, Prec: 0.847, Rec: 0.847, F1: 0.846\n",
            "Modelo 'VibrationCNN' salvo para o Fold 2.\n",
            "\n",
            "Fold 2: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6779, Val Loss: 0.6741, Val Acc: 0.5843\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6763, Val Loss: 0.6740, Val Acc: 0.5843\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6735, Val Loss: 0.6736, Val Acc: 0.5843\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6756, Val Loss: 0.6733, Val Acc: 0.5843\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6746, Val Loss: 0.6733, Val Acc: 0.5843\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6730, Val Loss: 0.6733, Val Acc: 0.5843\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6725, Val Loss: 0.6723, Val Acc: 0.5843\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6719, Val Loss: 0.6720, Val Acc: 0.5843\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6704, Val Loss: 0.6719, Val Acc: 0.5843\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6681, Val Loss: 0.6693, Val Acc: 0.5843\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6689, Val Loss: 0.6676, Val Acc: 0.5843\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6658, Val Loss: 0.6666, Val Acc: 0.5860\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6633, Val Loss: 0.6686, Val Acc: 0.5936\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6621, Val Loss: 0.6634, Val Acc: 0.5911\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6590, Val Loss: 0.6604, Val Acc: 0.5911\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6565, Val Loss: 0.6563, Val Acc: 0.5944\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6533, Val Loss: 0.6562, Val Acc: 0.5902\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6490, Val Loss: 0.6496, Val Acc: 0.5944\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6441, Val Loss: 0.6447, Val Acc: 0.5970\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6389, Val Loss: 0.6409, Val Acc: 0.6400\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6351, Val Loss: 0.6337, Val Acc: 0.6400\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6313, Val Loss: 0.6285, Val Acc: 0.6577\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6268, Val Loss: 0.6279, Val Acc: 0.6914\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6196, Val Loss: 0.6159, Val Acc: 0.6509\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6149, Val Loss: 0.6082, Val Acc: 0.6644\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6110, Val Loss: 0.6099, Val Acc: 0.6484\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6063, Val Loss: 0.6057, Val Acc: 0.7218\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.5922, Val Loss: 0.6076, Val Acc: 0.6046\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.5920, Val Loss: 0.5821, Val Acc: 0.7049\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.5821, Val Loss: 0.5874, Val Acc: 0.6374\n",
            "Fold 2 - Modelo 'SimCLR_LoRA_Classifier' - MÃ©tricas Finais (Val): Acc: 0.637, Prec: 0.711, Rec: 0.637, F1: 0.554\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 2.\n",
            "\n",
            "--- Fold 3 ---\n",
            "Fold 3: (Etapa 1) PrÃ©-treinando encoder para K-Means...\n",
            "Iniciando prÃ©-treinamento do encoder por 5 Ã©pocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0624\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0029\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0023\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0020\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0020\n",
            "PrÃ©-treinamento do encoder concluÃ­do.\n",
            "Fold 3: (Etapa 2) Extraindo embeddings...\n",
            "Fold 3: (Etapa 3) Executando K-Means...\n",
            "Fold 3: NÃºmero de classes estimado (best_k) = 2 (Silhouette: 0.206)\n",
            "Fold 3: (Etapa 4) Preparando modelos finais...\n",
            "Fold 3: Pesos do encoder prÃ©-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 3: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6765, Val Loss: 0.6739, Val Acc: 0.5911\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6708, Val Loss: 0.6680, Val Acc: 0.5911\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6685, Val Loss: 0.6661, Val Acc: 0.5911\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6681, Val Loss: 0.6647, Val Acc: 0.6003\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6679, Val Loss: 0.6632, Val Acc: 0.5927\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6634, Val Loss: 0.6636, Val Acc: 0.6223\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6614, Val Loss: 0.6580, Val Acc: 0.6214\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6593, Val Loss: 0.6537, Val Acc: 0.6526\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6552, Val Loss: 0.6508, Val Acc: 0.6172\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6510, Val Loss: 0.6399, Val Acc: 0.6728\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.6466, Val Loss: 0.6315, Val Acc: 0.6686\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.6372, Val Loss: 0.6223, Val Acc: 0.6459\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.6126, Val Loss: 0.6060, Val Acc: 0.7234\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.5904, Val Loss: 0.5692, Val Acc: 0.7083\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.5857, Val Loss: 0.5646, Val Acc: 0.7074\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.5608, Val Loss: 0.5975, Val Acc: 0.6509\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.5525, Val Loss: 0.5207, Val Acc: 0.7504\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.5268, Val Loss: 0.5526, Val Acc: 0.7150\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.5116, Val Loss: 0.5317, Val Acc: 0.7209\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.5025, Val Loss: 0.4825, Val Acc: 0.7774\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.5088, Val Loss: 0.5325, Val Acc: 0.7201\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.4839, Val Loss: 0.4763, Val Acc: 0.7690\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.4796, Val Loss: 0.5218, Val Acc: 0.7378\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.4756, Val Loss: 0.4815, Val Acc: 0.7673\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4760, Val Loss: 0.4688, Val Acc: 0.7681\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.4633, Val Loss: 0.4548, Val Acc: 0.7766\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4974, Val Loss: 0.4534, Val Acc: 0.7808\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.4804, Val Loss: 0.4569, Val Acc: 0.7707\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.4582, Val Loss: 0.4464, Val Acc: 0.7850\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.4540, Val Loss: 0.4717, Val Acc: 0.7673\n",
            "Fold 3 - Modelo 'VibrationCNN' - MÃ©tricas Finais (Val): Acc: 0.767, Prec: 0.783, Rec: 0.767, F1: 0.754\n",
            "Modelo 'VibrationCNN' salvo para o Fold 3.\n",
            "\n",
            "Fold 3: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6787, Val Loss: 0.6737, Val Acc: 0.5911\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6722, Val Loss: 0.6701, Val Acc: 0.5911\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6706, Val Loss: 0.6687, Val Acc: 0.5911\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6686, Val Loss: 0.6675, Val Acc: 0.5911\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6658, Val Loss: 0.6660, Val Acc: 0.5919\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6664, Val Loss: 0.6658, Val Acc: 0.5919\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6680, Val Loss: 0.6654, Val Acc: 0.5902\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6682, Val Loss: 0.6647, Val Acc: 0.5919\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6670, Val Loss: 0.6645, Val Acc: 0.5919\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6657, Val Loss: 0.6638, Val Acc: 0.5944\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6658, Val Loss: 0.6630, Val Acc: 0.5953\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6651, Val Loss: 0.6622, Val Acc: 0.6206\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6643, Val Loss: 0.6623, Val Acc: 0.6189\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6640, Val Loss: 0.6613, Val Acc: 0.5927\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6633, Val Loss: 0.6591, Val Acc: 0.6239\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6622, Val Loss: 0.6600, Val Acc: 0.6298\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6608, Val Loss: 0.6574, Val Acc: 0.5936\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6600, Val Loss: 0.6561, Val Acc: 0.6105\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6595, Val Loss: 0.6543, Val Acc: 0.5978\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6577, Val Loss: 0.6528, Val Acc: 0.6214\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6557, Val Loss: 0.6509, Val Acc: 0.6189\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6530, Val Loss: 0.6497, Val Acc: 0.6054\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6507, Val Loss: 0.6535, Val Acc: 0.5919\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6486, Val Loss: 0.6439, Val Acc: 0.6214\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6470, Val Loss: 0.6406, Val Acc: 0.6535\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6432, Val Loss: 0.6371, Val Acc: 0.6661\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6415, Val Loss: 0.6348, Val Acc: 0.6627\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6373, Val Loss: 0.6323, Val Acc: 0.6948\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6326, Val Loss: 0.6293, Val Acc: 0.6535\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6277, Val Loss: 0.6261, Val Acc: 0.6492\n",
            "Fold 3 - Modelo 'SimCLR_LoRA_Classifier' - MÃ©tricas Finais (Val): Acc: 0.649, Prec: 0.750, Rec: 0.649, F1: 0.563\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 3.\n",
            "\n",
            "--- Fold 4 ---\n",
            "Fold 4: (Etapa 1) PrÃ©-treinando encoder para K-Means...\n",
            "Iniciando prÃ©-treinamento do encoder por 5 Ã©pocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0887\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0048\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0031\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0027\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0025\n",
            "PrÃ©-treinamento do encoder concluÃ­do.\n",
            "Fold 4: (Etapa 2) Extraindo embeddings...\n",
            "Fold 4: (Etapa 3) Executando K-Means...\n",
            "Fold 4: NÃºmero de classes estimado (best_k) = 2 (Silhouette: 0.219)\n",
            "Fold 4: (Etapa 4) Preparando modelos finais...\n",
            "Fold 4: Pesos do encoder prÃ©-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 4: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6888, Val Loss: 0.6867, Val Acc: 0.5346\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6793, Val Loss: 0.6858, Val Acc: 0.5430\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6799, Val Loss: 0.6847, Val Acc: 0.5388\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6776, Val Loss: 0.6975, Val Acc: 0.5641\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6780, Val Loss: 0.6833, Val Acc: 0.5531\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6788, Val Loss: 0.6848, Val Acc: 0.5691\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6775, Val Loss: 0.6815, Val Acc: 0.5481\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6741, Val Loss: 0.6801, Val Acc: 0.5472\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6740, Val Loss: 0.6780, Val Acc: 0.5548\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6719, Val Loss: 0.6780, Val Acc: 0.5784\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.6701, Val Loss: 0.6703, Val Acc: 0.5691\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.6655, Val Loss: 0.6772, Val Acc: 0.5961\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.6607, Val Loss: 0.6636, Val Acc: 0.5894\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.6567, Val Loss: 0.6438, Val Acc: 0.6239\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.6476, Val Loss: 0.6511, Val Acc: 0.6265\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.6335, Val Loss: 0.6083, Val Acc: 0.6543\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.6006, Val Loss: 0.6699, Val Acc: 0.5472\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.6138, Val Loss: 0.5594, Val Acc: 0.7192\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.5631, Val Loss: 0.8116, Val Acc: 0.5236\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.5576, Val Loss: 0.4959, Val Acc: 0.7715\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.5286, Val Loss: 0.4773, Val Acc: 0.7833\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.5246, Val Loss: 0.5428, Val Acc: 0.6897\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.5099, Val Loss: 0.5036, Val Acc: 0.7243\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.5091, Val Loss: 0.4503, Val Acc: 0.7909\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4887, Val Loss: 0.4575, Val Acc: 0.7858\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.4901, Val Loss: 0.4495, Val Acc: 0.7934\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4793, Val Loss: 0.4588, Val Acc: 0.7841\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.4836, Val Loss: 0.4217, Val Acc: 0.8103\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.4705, Val Loss: 0.4474, Val Acc: 0.7723\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.4697, Val Loss: 0.4135, Val Acc: 0.7917\n",
            "Fold 4 - Modelo 'VibrationCNN' - MÃ©tricas Finais (Val): Acc: 0.792, Prec: 0.799, Rec: 0.792, F1: 0.790\n",
            "Modelo 'VibrationCNN' salvo para o Fold 4.\n",
            "\n",
            "Fold 4: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6916, Val Loss: 0.6893, Val Acc: 0.5472\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6842, Val Loss: 0.6859, Val Acc: 0.5405\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6813, Val Loss: 0.6844, Val Acc: 0.5371\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6791, Val Loss: 0.6839, Val Acc: 0.5481\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6771, Val Loss: 0.6829, Val Acc: 0.5430\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6777, Val Loss: 0.6818, Val Acc: 0.5430\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6756, Val Loss: 0.6816, Val Acc: 0.5497\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6744, Val Loss: 0.6803, Val Acc: 0.5531\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6757, Val Loss: 0.6790, Val Acc: 0.5531\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6732, Val Loss: 0.6820, Val Acc: 0.5632\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6723, Val Loss: 0.6773, Val Acc: 0.5548\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6720, Val Loss: 0.6766, Val Acc: 0.5616\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6706, Val Loss: 0.6751, Val Acc: 0.5624\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6690, Val Loss: 0.6799, Val Acc: 0.5818\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6664, Val Loss: 0.6709, Val Acc: 0.5978\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6652, Val Loss: 0.6710, Val Acc: 0.5978\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6650, Val Loss: 0.6684, Val Acc: 0.5700\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6635, Val Loss: 0.6674, Val Acc: 0.5750\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6595, Val Loss: 0.6655, Val Acc: 0.6096\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6597, Val Loss: 0.6582, Val Acc: 0.6164\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6532, Val Loss: 0.6537, Val Acc: 0.6180\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6519, Val Loss: 0.6665, Val Acc: 0.5826\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6486, Val Loss: 0.6423, Val Acc: 0.6594\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6431, Val Loss: 0.6500, Val Acc: 0.6164\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6400, Val Loss: 0.6356, Val Acc: 0.6745\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6335, Val Loss: 0.6339, Val Acc: 0.6484\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6331, Val Loss: 0.6231, Val Acc: 0.6821\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6257, Val Loss: 0.6173, Val Acc: 0.7007\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6230, Val Loss: 0.6179, Val Acc: 0.6771\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6168, Val Loss: 0.6216, Val Acc: 0.6543\n",
            "Fold 4 - Modelo 'SimCLR_LoRA_Classifier' - MÃ©tricas Finais (Val): Acc: 0.654, Prec: 0.677, Rec: 0.654, F1: 0.642\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 4.\n",
            "\n",
            "--- Fold 5 ---\n",
            "Fold 5: (Etapa 1) PrÃ©-treinando encoder para K-Means...\n",
            "Iniciando prÃ©-treinamento do encoder por 5 Ã©pocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0703\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0036\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0027\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0023\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0020\n",
            "PrÃ©-treinamento do encoder concluÃ­do.\n",
            "Fold 5: (Etapa 2) Extraindo embeddings...\n",
            "Fold 5: (Etapa 3) Executando K-Means...\n",
            "Fold 5: NÃºmero de classes estimado (best_k) = 2 (Silhouette: 0.167)\n",
            "Fold 5: (Etapa 4) Preparando modelos finais...\n",
            "Fold 5: Pesos do encoder prÃ©-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 5: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6933, Val Loss: 0.6941, Val Acc: 0.4895\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6931, Val Loss: 0.6928, Val Acc: 0.5232\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6934, Val Loss: 0.6936, Val Acc: 0.5030\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6925, Val Loss: 0.6928, Val Acc: 0.5215\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6929, Val Loss: 0.6955, Val Acc: 0.4895\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6925, Val Loss: 0.6921, Val Acc: 0.5232\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6920, Val Loss: 0.6938, Val Acc: 0.5131\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6914, Val Loss: 0.6931, Val Acc: 0.5148\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6898, Val Loss: 0.6884, Val Acc: 0.5536\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6873, Val Loss: 0.6836, Val Acc: 0.6000\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.6792, Val Loss: 0.6837, Val Acc: 0.5392\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.6671, Val Loss: 0.6941, Val Acc: 0.4928\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.6506, Val Loss: 0.6316, Val Acc: 0.7156\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.6305, Val Loss: 0.6173, Val Acc: 0.6616\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.5917, Val Loss: 0.5640, Val Acc: 0.7502\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.5647, Val Loss: 0.5688, Val Acc: 0.7173\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.5557, Val Loss: 0.5288, Val Acc: 0.7629\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.5287, Val Loss: 0.5467, Val Acc: 0.7215\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.5159, Val Loss: 0.5386, Val Acc: 0.7114\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.4974, Val Loss: 0.5185, Val Acc: 0.7367\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.4969, Val Loss: 0.4947, Val Acc: 0.7637\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.4902, Val Loss: 0.7772, Val Acc: 0.6008\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.4860, Val Loss: 0.4959, Val Acc: 0.7637\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.4702, Val Loss: 0.5456, Val Acc: 0.7046\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4587, Val Loss: 0.4623, Val Acc: 0.7831\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.4548, Val Loss: 0.4569, Val Acc: 0.7848\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4660, Val Loss: 0.4997, Val Acc: 0.7460\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.4517, Val Loss: 0.5092, Val Acc: 0.7485\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.4386, Val Loss: 0.4487, Val Acc: 0.7882\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.4463, Val Loss: 0.4807, Val Acc: 0.7662\n",
            "Fold 5 - Modelo 'VibrationCNN' - MÃ©tricas Finais (Val): Acc: 0.766, Prec: 0.785, Rec: 0.766, F1: 0.763\n",
            "Modelo 'VibrationCNN' salvo para o Fold 5.\n",
            "\n",
            "Fold 5: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6935, Val Loss: 0.6938, Val Acc: 0.4895\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6931, Val Loss: 0.6933, Val Acc: 0.5063\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6930, Val Loss: 0.6934, Val Acc: 0.4970\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6929, Val Loss: 0.6933, Val Acc: 0.5072\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6922, Val Loss: 0.6923, Val Acc: 0.5181\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6921, Val Loss: 0.6915, Val Acc: 0.5274\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6916, Val Loss: 0.6919, Val Acc: 0.5392\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6906, Val Loss: 0.6925, Val Acc: 0.5257\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6907, Val Loss: 0.6901, Val Acc: 0.5468\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6891, Val Loss: 0.6885, Val Acc: 0.5603\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6879, Val Loss: 0.6879, Val Acc: 0.5586\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6859, Val Loss: 0.6893, Val Acc: 0.5359\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6846, Val Loss: 0.6834, Val Acc: 0.5477\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6819, Val Loss: 0.6795, Val Acc: 0.6692\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6767, Val Loss: 0.6755, Val Acc: 0.6481\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6726, Val Loss: 0.6769, Val Acc: 0.5882\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6694, Val Loss: 0.6666, Val Acc: 0.6549\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6640, Val Loss: 0.6734, Val Acc: 0.5781\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6615, Val Loss: 0.6796, Val Acc: 0.5173\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6589, Val Loss: 0.6597, Val Acc: 0.6734\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6522, Val Loss: 0.6487, Val Acc: 0.6700\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6462, Val Loss: 0.6544, Val Acc: 0.6473\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6359, Val Loss: 0.6548, Val Acc: 0.5443\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6357, Val Loss: 0.6327, Val Acc: 0.7089\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6320, Val Loss: 0.6286, Val Acc: 0.7105\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6212, Val Loss: 0.6222, Val Acc: 0.7257\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6167, Val Loss: 0.6286, Val Acc: 0.6819\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6109, Val Loss: 0.6103, Val Acc: 0.7266\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6025, Val Loss: 0.6044, Val Acc: 0.7198\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6039, Val Loss: 0.6016, Val Acc: 0.7401\n",
            "Fold 5 - Modelo 'SimCLR_LoRA_Classifier' - MÃ©tricas Finais (Val): Acc: 0.740, Prec: 0.744, Rec: 0.740, F1: 0.739\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 5.\n",
            "\n",
            "--- ValidaÃ§Ã£o Cruzada ConcluÃ­da ---\n",
            "\n",
            "AnÃ¡lise concluÃ­da.\n",
            "Scores F1 finais (do Ãºltimo fold processado com sucesso):\n",
            "  VibrationCNN: 0.7632\n",
            "  SimCLR_LoRA_Classifier: 0.7395\n",
            "Melhor modelo no Ãºltimo fold (baseado em F1): VibrationCNN (F1: 0.7632)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGP--ZjLCw0R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}