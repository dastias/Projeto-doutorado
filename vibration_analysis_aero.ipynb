{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMwB7MEykrwvYArmAAo2wUy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dastias/Projeto-doutorado/blob/main/vibration_analysis_aero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqR8fb6N4y88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b42aae5-e1f4-40e1-b1b0-97f08d5f8728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyWavelets"
      ],
      "metadata": {
        "id": "YAtBPjt_tvKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf66decf-e7e9-4a62-aa8f-0fa8ad1b046d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.11/dist-packages (1.8.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from PyWavelets) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vibration_analysis_aero.py - Código completo corrigido\n",
        "\n",
        "import os, math\n",
        "import numpy as np\n",
        "import pywt\n",
        "from scipy import signal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "class SignalPreprocessor:\n",
        "    def __init__(self, window_size=1024, overlap=0.5, sampling_rate=12800):\n",
        "        self.window_size = window_size\n",
        "        self.overlap = overlap\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "    def remove_noise(self, data: np.ndarray) -> np.ndarray:\n",
        "        coeffs = pywt.wavedec(data, 'db4', level=4)\n",
        "        thr = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = pywt.threshold(coeffs[i], thr, mode='soft')\n",
        "        return pywt.waverec(coeffs, 'db4')\n",
        "\n",
        "    def generate_spectrogram(self, data: np.ndarray) -> np.ndarray:\n",
        "        nperseg = self.window_size\n",
        "        noverlap = int(nperseg * self.overlap)\n",
        "        if noverlap >= nperseg:\n",
        "            noverlap = nperseg - 1\n",
        "        _, _, Sxx = signal.spectrogram(\n",
        "            data,\n",
        "            fs=self.sampling_rate,\n",
        "            window='hann',\n",
        "            nperseg=nperseg,\n",
        "            noverlap=noverlap\n",
        "        )\n",
        "        Sxx = 10 * np.log10(Sxx + 1e-10)\n",
        "        return (Sxx - Sxx.min()) / (Sxx.max() - Sxx.min() + 1e-10)\n",
        "\n",
        "class UnsupervisedAeroDataset(Dataset):\n",
        "    def __init__(self, csv_paths, transform=None):\n",
        "        self.transform = transform\n",
        "        self.pre = SignalPreprocessor()\n",
        "        self.samples = []\n",
        "        self.raw_signals = []\n",
        "\n",
        "        for path in csv_paths:\n",
        "            data = np.loadtxt(path, delimiter=',')\n",
        "            for row in data:\n",
        "                x = row[2:]\n",
        "                den = self.pre.remove_noise(x)\n",
        "                spec = self.pre.generate_spectrogram(den)\n",
        "                tensor = torch.tensor(spec).float().unsqueeze(0)\n",
        "                if self.transform:\n",
        "                    tensor = self.transform(tensor)\n",
        "                self.samples.append(tensor)\n",
        "                self.raw_signals.append(x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx], self.raw_signals[idx], self.pre.remove_noise(self.raw_signals[idx])\n",
        "\n",
        "class VibrationAnalysisCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(), nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        x = self.features(x).view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "class LoRAAdapter(nn.Module):\n",
        "    def __init__(self, layer: nn.Linear, rank=4, scale=0.01):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "        self.layer.weight.requires_grad = False\n",
        "        if self.layer.bias is not None:\n",
        "            self.layer.bias.requires_grad = False\n",
        "\n",
        "        in_f, out_f = layer.in_features, layer.out_features\n",
        "        self.A = nn.Parameter(torch.zeros(in_f, rank))\n",
        "        self.B = nn.Parameter(torch.zeros(rank, out_f))\n",
        "        self.scale = scale\n",
        "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x) + (x @ self.A @ self.B) * self.scale\n",
        "\n",
        "class SimCLRModelLoRA(nn.Module):\n",
        "    def __init__(self, projection_dim=128, num_classes=2, lora_rank=4, lora_scale=0.01):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(), nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, projection_dim)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            LoRAAdapter(nn.Linear(128, 256), rank=lora_rank, scale=lora_scale),\n",
        "            nn.ReLU(), nn.Dropout(0.5),\n",
        "            LoRAAdapter(nn.Linear(256, num_classes), rank=lora_rank, scale=lora_scale)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.float()\n",
        "        h = self.encoder(x).view(x.size(0), -1)\n",
        "        return self.projection(h)\n",
        "\n",
        "    def forward_classifier(self, x):\n",
        "        x = x.float()\n",
        "        h = self.encoder(x).view(x.size(0), -1)\n",
        "        return self.classifier(h)\n",
        "\n",
        "class ModelAgent:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scores = {}\n",
        "\n",
        "    def update_models(self, models: dict):\n",
        "        self.models = models\n",
        "        self.scores = {k: 1.0 for k in models}\n",
        "\n",
        "    def update_performance(self, name, score):\n",
        "        self.scores[name] = 0.9 * self.scores[name] + 0.1 * score\n",
        "\n",
        "    def select_best_model(self):\n",
        "        return max(self.scores, key=self.scores.get)\n",
        "\n",
        "class VibrationAnalyzer:\n",
        "    def __init__(self, config, agent):\n",
        "        self.config = config\n",
        "        self.agent = agent\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.pre = SignalPreprocessor()\n",
        "\n",
        "    def pretrain_simclr(self, loader):\n",
        "        model = self.agent.models['simclr_lora'].to(self.device)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=self.config['learning_rate'])\n",
        "        model.train()\n",
        "        for _ in range(self.config['simclr_epochs']):\n",
        "            for spec, _, _ in loader:\n",
        "                spec = spec.to(self.device).float()\n",
        "                opt.zero_grad()\n",
        "                loss = torch.norm(model(spec), dim=1).mean()\n",
        "                loss.backward()\n",
        "                opt.step()\n",
        "\n",
        "    def extract_embeddings(self, loader):\n",
        "        model = self.agent.models['simclr_lora'].to(self.device).eval()\n",
        "        embs = []\n",
        "        with torch.no_grad():\n",
        "            for spec, _, _ in loader:\n",
        "                embs.append(model(spec.to(self.device).float()).cpu().numpy())\n",
        "        return np.vstack(embs)\n",
        "\n",
        "    def train_and_eval_fold(self, train_ds, val_ds, fold_idx):\n",
        "        # etapa temporária para extrair embeddings com modelo default\n",
        "        self.agent.update_models({\n",
        "            'simclr_lora': SimCLRModelLoRA(projection_dim=128, num_classes=2)\n",
        "        })\n",
        "        loader_all = DataLoader(train_ds, batch_size=self.config['batch_size'], shuffle=False)\n",
        "        embs = self.extract_embeddings(loader_all)\n",
        "\n",
        "        best_k, best_s = 2, -1\n",
        "        for k in range(2, 11):\n",
        "            km = KMeans(n_clusters=k, random_state=42).fit(embs)\n",
        "            s = silhouette_score(embs, km.labels_)\n",
        "            if s > best_s:\n",
        "                best_k, best_s = k, s\n",
        "\n",
        "        self.agent.update_models({\n",
        "            'cnn': VibrationAnalysisCNN(num_classes=best_k),\n",
        "            'simclr_lora': SimCLRModelLoRA(projection_dim=128, num_classes=best_k)\n",
        "        })\n",
        "\n",
        "        self.pretrain_simclr(DataLoader(train_ds, batch_size=self.config['batch_size'], shuffle=True))\n",
        "\n",
        "        km = KMeans(n_clusters=best_k, random_state=42).fit(embs)\n",
        "        pseudo = km.labels_\n",
        "\n",
        "        class PLDS(Dataset):\n",
        "            def __init__(self, base_ds, labels):\n",
        "                self.base, self.labels = base_ds, labels\n",
        "            def __len__(self): return len(self.base)\n",
        "            def __getitem__(self, i):\n",
        "                spec, raw, den = self.base[i]\n",
        "                return spec, self.labels[i], raw, den\n",
        "\n",
        "        sup_tr = PLDS(train_ds, pseudo)\n",
        "        sup_va = PLDS(val_ds, pseudo)\n",
        "\n",
        "        for name, model in self.agent.models.items():\n",
        "            model = model.to(self.device)\n",
        "            opt = torch.optim.Adam(model.parameters(), lr=self.config['learning_rate'])\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "            history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "            for ep in range(self.config['epochs']):\n",
        "                model.train(); tl = 0\n",
        "                for spec, label, _, _ in DataLoader(sup_tr, batch_size=self.config['batch_size'], shuffle=True):\n",
        "                    spec = spec.to(self.device).float()\n",
        "                    label = label.to(self.device).long()\n",
        "                    opt.zero_grad()\n",
        "                    out = model(spec) if name == 'cnn' else model.forward_classifier(spec)\n",
        "                    loss = loss_fn(out, label)\n",
        "                    loss.backward(); opt.step()\n",
        "                    tl += loss.item()\n",
        "                history['train_loss'].append(tl / len(sup_tr))\n",
        "\n",
        "                model.eval(); vl, corr = 0, 0\n",
        "                with torch.no_grad():\n",
        "                    for spec, label, _, _ in DataLoader(sup_va, batch_size=self.config['batch_size']):\n",
        "                        spec = spec.to(self.device).float()\n",
        "                        label = label.to(self.device).long()\n",
        "                        out = model(spec) if name == 'cnn' else model.forward_classifier(spec)\n",
        "                        vl += loss_fn(out, label).item()\n",
        "                        preds = out.argmax(dim=1)\n",
        "                        corr += (preds == label).sum().item()\n",
        "                history['val_loss'].append(vl / len(sup_va))\n",
        "                history['val_acc'].append(corr / len(sup_va))\n",
        "\n",
        "            # métricas finais\n",
        "            all_preds, all_labels = [], []\n",
        "            with torch.no_grad():\n",
        "                for spec, label, _, _ in DataLoader(sup_va, batch_size=self.config['batch_size']):\n",
        "                    out = model(spec.to(self.device).float()) if name == 'cnn' else model.forward_classifier(spec.to(self.device).float())\n",
        "                    preds = out.argmax(dim=1).cpu().numpy()\n",
        "                    labs = label.cpu().numpy()\n",
        "                    all_preds.extend(preds)\n",
        "                    all_labels.extend(labs)\n",
        "\n",
        "            metrics = {\n",
        "                'accuracy': accuracy_score(all_labels, all_preds),\n",
        "                'precision': precision_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
        "                'recall': recall_score(all_labels, all_preds, average='weighted', zero_division=0),\n",
        "                'f1': f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "            }\n",
        "            print(f\"{name} – acc {metrics['accuracy']:.3f}, prec {metrics['precision']:.3f}, rec {metrics['recall']:.3f}, f1 {metrics['f1']:.3f}\")\n",
        "            self.agent.update_performance(name, metrics['f1'])\n",
        "            pd.DataFrame([metrics]).to_csv(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_metrics.csv'), index=False)\n",
        "            dfh = pd.DataFrame(history)\n",
        "            dfh['epoch'] = np.arange(1, self.config['epochs'] + 1)\n",
        "            dfh.to_csv(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_history.csv'), index=False)\n",
        "            plt.figure()\n",
        "            plt.plot(dfh['epoch'], dfh['train_loss'], label='Train Loss')\n",
        "            plt.plot(dfh['epoch'], dfh['val_loss'], label='Val Loss')\n",
        "            plt.legend(); plt.title(f'Loss Curve – {name}')\n",
        "            plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_loss.png'))\n",
        "            plt.close()\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(dfh['epoch'], dfh['val_acc'], label='Val Accuracy')\n",
        "            plt.legend(); plt.title(f'Accuracy Curve – {name}')\n",
        "            plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_acc.png'))\n",
        "            plt.close()\n",
        "\n",
        "            cm = confusion_matrix(all_labels, all_preds)\n",
        "            plt.figure(figsize=(6,5))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "            plt.title(f'Matriz de Confusão – {name}')\n",
        "            plt.xlabel('Predito')\n",
        "            plt.ylabel('Real')\n",
        "            plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_cm.png'))\n",
        "            plt.close()\n",
        "\n",
        "            torch.save(model.state_dict(), os.path.join(self.config['output_dir'], f'fold{fold_idx}_{name}_model.pt'))\n",
        "\n",
        "    def cross_validate(self, dataset):\n",
        "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        for i, (tr, va) in enumerate(kf.split(dataset)):\n",
        "            self.train_and_eval_fold(Subset(dataset, tr), Subset(dataset, va), i)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    from torchvision import transforms\n",
        "\n",
        "    config = {\n",
        "        'learning_rate': 1e-3,\n",
        "        'simclr_epochs': 5,\n",
        "        'batch_size': 32,\n",
        "        'epochs': 10,\n",
        "        'output_dir': '/content/drive/My Drive/aero_results/'\n",
        "    }\n",
        "    os.makedirs(config['output_dir'], exist_ok=True)\n",
        "\n",
        "    tfm = transforms.Normalize([0.5], [0.5])\n",
        "    csvs = [\n",
        "        '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET06.csv',\n",
        "        '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET07.csv'\n",
        "    ]\n",
        "    ds = UnsupervisedAeroDataset(csvs, transform=tfm)\n",
        "\n",
        "    agent = ModelAgent()\n",
        "    analyzer = VibrationAnalyzer(config, agent)\n",
        "    analyzer.cross_validate(ds)\n"
      ],
      "metadata": {
        "id": "vM45-UPNxlds",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447a6028-acbf-4c11-d9c8-28059b587a27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cnn – acc 0.562, prec 0.612, rec 0.562, f1 0.582\n",
            "simclr_lora – acc 0.578, prec 0.603, rec 0.578, f1 0.589\n",
            "cnn – acc 0.501, prec 0.545, rec 0.501, f1 0.514\n",
            "simclr_lora – acc 0.517, prec 0.549, rec 0.517, f1 0.528\n",
            "cnn – acc 0.547, prec 0.603, rec 0.547, f1 0.567\n",
            "simclr_lora – acc 0.557, prec 0.595, rec 0.557, f1 0.572\n",
            "cnn – acc 0.588, prec 0.639, rec 0.588, f1 0.608\n",
            "simclr_lora – acc 0.589, prec 0.641, rec 0.589, f1 0.609\n",
            "cnn – acc 0.542, prec 0.617, rec 0.542, f1 0.570\n",
            "simclr_lora – acc 0.559, prec 0.617, rec 0.559, f1 0.582\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# vibration_analysis_aero.py - Código completo corrigido e com sugestões implementadas\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pywt\n",
        "from scipy import signal\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torchvision import transforms # Movido para o topo para melhor organização\n",
        "\n",
        "# Helper para garantir que os diretórios de output existem\n",
        "def ensure_dir(directory):\n",
        "    if not os.path.exists(directory):\n",
        "        os.makedirs(directory)\n",
        "\n",
        "class SignalPreprocessor:\n",
        "    def __init__(self, window_size=1024, overlap=0.5, sampling_rate=12800):\n",
        "        self.window_size = window_size\n",
        "        self.overlap = overlap\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "    def remove_noise(self, data: np.ndarray) -> np.ndarray:\n",
        "        coeffs = pywt.wavedec(data, 'db4', level=4)\n",
        "        sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
        "        thr = sigma * np.sqrt(2 * np.log(len(data))) if len(data) > 1 else 0 # Limiar de Donoho\n",
        "        for i in range(1, len(coeffs)):\n",
        "            coeffs[i] = pywt.threshold(coeffs[i], thr, mode='soft')\n",
        "        reconstructed_signal = pywt.waverec(coeffs, 'db4')\n",
        "        if len(reconstructed_signal) != len(data):\n",
        "            reconstructed_signal = reconstructed_signal[:len(data)]\n",
        "        return reconstructed_signal\n",
        "\n",
        "    def generate_spectrogram(self, data: np.ndarray) -> np.ndarray:\n",
        "        nperseg = self.window_size\n",
        "        noverlap = int(nperseg * self.overlap)\n",
        "        if noverlap >= nperseg:\n",
        "            noverlap = nperseg - 1\n",
        "\n",
        "        if len(data) < nperseg:\n",
        "            padding_length = nperseg - len(data)\n",
        "            data = np.pad(data, (0, padding_length), 'constant', constant_values=(0,0))\n",
        "\n",
        "        _, _, Sxx = signal.spectrogram(\n",
        "            data,\n",
        "            fs=self.sampling_rate,\n",
        "            window='hann',\n",
        "            nperseg=nperseg,\n",
        "            noverlap=noverlap\n",
        "        )\n",
        "        Sxx = 10 * np.log10(Sxx + 1e-10)\n",
        "        min_sxx = Sxx.min()\n",
        "        max_sxx = Sxx.max()\n",
        "        if max_sxx - min_sxx < 1e-10:\n",
        "            return np.zeros_like(Sxx)\n",
        "        return (Sxx - min_sxx) / (max_sxx - min_sxx + 1e-10)\n",
        "\n",
        "class UnsupervisedAeroDataset(Dataset):\n",
        "    def __init__(self, csv_paths, signal_preprocessor: SignalPreprocessor, transform=None):\n",
        "        self.transform = transform\n",
        "        self.pre = signal_preprocessor\n",
        "        self.samples = []\n",
        "\n",
        "        print(f\"Carregando e pré-processando dados de: {csv_paths}\")\n",
        "        for path_idx, path in enumerate(csv_paths):\n",
        "            try:\n",
        "                raw_data_file = pd.read_csv(path, header=None, low_memory=False)\n",
        "                for row_idx, row_series in raw_data_file.iterrows():\n",
        "                    try:\n",
        "                        signal_data = pd.to_numeric(row_series.iloc[2:], errors='coerce').to_numpy()\n",
        "                        signal_data = signal_data[~np.isnan(signal_data)]\n",
        "\n",
        "                        if len(signal_data) < self.pre.window_size :\n",
        "                            continue\n",
        "\n",
        "                        denoised_signal = self.pre.remove_noise(signal_data)\n",
        "\n",
        "                        if len(denoised_signal) < self.pre.window_size:\n",
        "                             continue\n",
        "\n",
        "                        spectrogram = self.pre.generate_spectrogram(denoised_signal)\n",
        "                        tensor = torch.tensor(spectrogram, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "                        if self.transform:\n",
        "                            tensor = self.transform(tensor)\n",
        "\n",
        "                        self.samples.append(tensor)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Erro processando linha {row_idx+1} do arquivo {path}: {e}. Pulando linha.\")\n",
        "                        continue\n",
        "            except Exception as e:\n",
        "                print(f\"Erro lendo ou processando arquivo CSV {path}: {e}. Pulando arquivo.\")\n",
        "                continue\n",
        "        print(f\"Total de {len(self.samples)} amostras carregadas.\")\n",
        "        if not self.samples:\n",
        "            raise ValueError(\"Nenhuma amostra foi carregada. Verifique os arquivos CSV e o pré-processamento.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.samples[idx]\n",
        "\n",
        "\n",
        "class PLDS(Dataset): # Pseudo-Label Dataset\n",
        "    def __init__(self, base_subset: Subset, labels: np.ndarray):\n",
        "        self.base_subset = base_subset\n",
        "        self.labels = labels\n",
        "        assert len(self.base_subset) == len(self.labels), \\\n",
        "            f\"Dataset (len: {len(self.base_subset)}) e labels (len: {len(self.labels)}) devem ter o mesmo tamanho.\"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.base_subset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        spectrogram_tensor = self.base_subset[idx]\n",
        "        label = self.labels[idx]\n",
        "        # Retorna apenas os dados relevantes\n",
        "        return spectrogram_tensor, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "\n",
        "class VibrationAnalysisCNN(nn.Module):\n",
        "    def __init__(self, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "class LoRAAdapter(nn.Module):\n",
        "    def __init__(self, layer: nn.Linear, rank=4, scale=0.01):\n",
        "        super().__init__()\n",
        "        self.layer = layer\n",
        "        self.layer.weight.requires_grad = False\n",
        "        if self.layer.bias is not None:\n",
        "            self.layer.bias.requires_grad = False\n",
        "\n",
        "        in_f, out_f = layer.in_features, layer.out_features\n",
        "        self.A = nn.Parameter(torch.Tensor(in_f, rank))\n",
        "        self.B = nn.Parameter(torch.Tensor(rank, out_f))\n",
        "        self.scale = scale\n",
        "\n",
        "        nn.init.kaiming_uniform_(self.A, a=math.sqrt(5))\n",
        "        nn.init.zeros_(self.B)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer(x) + (x @ self.A @ self.B) * self.scale\n",
        "\n",
        "\n",
        "class SimCLRModelLoRA(nn.Module):\n",
        "    def __init__(self, projection_dim=128, num_classes=2, lora_rank=4, lora_scale=0.01):\n",
        "        super().__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1))\n",
        "        )\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(128, 256), nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, projection_dim)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            LoRAAdapter(nn.Linear(128, 256), rank=lora_rank, scale=lora_scale),\n",
        "            nn.ReLU(inplace=True), nn.Dropout(0.5),\n",
        "            LoRAAdapter(nn.Linear(256, num_classes), rank=lora_rank, scale=lora_scale)\n",
        "        )\n",
        "\n",
        "    def forward_encoder_projection(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        return self.projection(h)\n",
        "\n",
        "    def forward_classifier(self, x):\n",
        "        h = self.encoder(x)\n",
        "        h = h.view(h.size(0), -1)\n",
        "        return self.classifier(h)\n",
        "\n",
        "\n",
        "class ModelAgent:\n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scores = {}\n",
        "\n",
        "    def update_models(self, models: dict):\n",
        "        self.models = models\n",
        "        self.scores = {name: 0.0 for name in models}\n",
        "\n",
        "    def update_performance(self, model_name, score):\n",
        "        self.scores[model_name] = score\n",
        "\n",
        "    def get_best_model_name(self):\n",
        "        if not self.scores:\n",
        "            return None\n",
        "        return max(self.scores, key=self.scores.get)\n",
        "\n",
        "\n",
        "class VibrationAnalyzer:\n",
        "    def __init__(self, config, agent: ModelAgent, signal_preprocessor: SignalPreprocessor):\n",
        "        self.config = config\n",
        "        self.agent = agent\n",
        "        self.signal_preprocessor = signal_preprocessor\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Usando dispositivo: {self.device}\")\n",
        "        ensure_dir(self.config['output_dir'])\n",
        "\n",
        "    def _train_encoder_module(self, model_to_train: SimCLRModelLoRA, loader: DataLoader, epochs: int, learning_rate: float):\n",
        "        print(f\"Iniciando pré-treinamento do encoder por {epochs} épocas...\")\n",
        "        model_to_train.to(self.device)\n",
        "        optimizer = torch.optim.Adam(list(model_to_train.encoder.parameters()) + list(model_to_train.projection.parameters()), lr=learning_rate)\n",
        "\n",
        "        model_to_train.train()\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            for batch_idx, spectrograms in enumerate(loader): # CORRIGIDO AQUI\n",
        "                spectrograms = spectrograms.to(self.device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                projections = model_to_train.forward_encoder_projection(spectrograms)\n",
        "                loss = torch.norm(projections, p=2, dim=1).mean()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "            avg_loss = total_loss / len(loader) if len(loader) > 0 else 0\n",
        "            print(f\"Encoder Pretrain Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "        print(\"Pré-treinamento do encoder concluído.\")\n",
        "\n",
        "    def _extract_embeddings_from_module(self, model_extractor: SimCLRModelLoRA, loader: DataLoader) -> np.ndarray:\n",
        "        model_extractor.to(self.device)\n",
        "        model_extractor.eval()\n",
        "        all_embeddings = []\n",
        "        with torch.no_grad():\n",
        "            for spectrograms in loader: # CORRIGIDO AQUI\n",
        "                spectrograms = spectrograms.to(self.device)\n",
        "                embeddings = model_extractor.forward_encoder_projection(spectrograms)\n",
        "                all_embeddings.append(embeddings.cpu().numpy())\n",
        "        if not all_embeddings: # Lidar com caso de lista vazia\n",
        "            return np.array([])\n",
        "        return np.vstack(all_embeddings)\n",
        "\n",
        "    def train_and_eval_fold(self, train_subset: Subset, val_subset: Subset, fold_idx: int):\n",
        "        print(f\"\\n--- Fold {fold_idx + 1} ---\")\n",
        "\n",
        "        # ETAPA 1: Pré-treinamento do Encoder para Extração de Embeddings\n",
        "        print(f\"Fold {fold_idx + 1}: (Etapa 1) Pré-treinando encoder para K-Means...\")\n",
        "        encoder_trainer_model = SimCLRModelLoRA(\n",
        "            projection_dim=self.config['projection_dim_simclr'],\n",
        "            num_classes=2\n",
        "        ).to(self.device)\n",
        "\n",
        "        pretrain_loader = DataLoader(train_subset, batch_size=self.config['batch_size'], shuffle=True,\n",
        "                                     num_workers=self.config.get('num_workers', 0), pin_memory=True) # num_workers=0 para debug no Colab se necessário\n",
        "\n",
        "        if len(pretrain_loader) > 0:\n",
        "            self._train_encoder_module(encoder_trainer_model, pretrain_loader,\n",
        "                                    self.config['simclr_epochs_for_kmeans'], self.config['learning_rate_simclr_kmeans'])\n",
        "        else:\n",
        "            print(f\"Fold {fold_idx + 1}: Pretrain_loader vazio. Pulando pré-treinamento do encoder.\")\n",
        "\n",
        "\n",
        "        # ETAPA 2: Extração de Embeddings com o Encoder Pré-treinado\n",
        "        print(f\"Fold {fold_idx + 1}: (Etapa 2) Extraindo embeddings...\")\n",
        "        emb_loader_train = DataLoader(train_subset, batch_size=self.config['batch_size'], shuffle=False,\n",
        "                                      num_workers=self.config.get('num_workers', 0), pin_memory=True)\n",
        "        emb_loader_val = DataLoader(val_subset, batch_size=self.config['batch_size'], shuffle=False,\n",
        "                                    num_workers=self.config.get('num_workers', 0), pin_memory=True)\n",
        "\n",
        "        train_embeddings = self._extract_embeddings_from_module(encoder_trainer_model, emb_loader_train)\n",
        "        val_embeddings = self._extract_embeddings_from_module(encoder_trainer_model, emb_loader_val)\n",
        "\n",
        "        if len(train_embeddings) == 0:\n",
        "            print(f\"Fold {fold_idx + 1}: Nenhum embedding de treino extraído. Pulando fold.\")\n",
        "            return\n",
        "\n",
        "        # ETAPA 3: K-Means para Número de Classes e Pseudo-Rótulos\n",
        "        print(f\"Fold {fold_idx + 1}: (Etapa 3) Executando K-Means...\")\n",
        "        best_k, best_silhouette_score = 2, -1.0\n",
        "\n",
        "        max_possible_k = min(self.config['max_k_means_clusters'], len(train_embeddings))\n",
        "\n",
        "        if max_possible_k < 2:\n",
        "            print(f\"Fold {fold_idx + 1}: Número insuficiente de amostras de treino ({len(train_embeddings)}) para K-Means com k >= 2. Usando k=2 por padrão.\")\n",
        "            best_k = 2\n",
        "            if len(train_embeddings) < 2 :\n",
        "                 print(f\"Fold {fold_idx + 1}: Menos de 2 amostras de treino. Impossível prosseguir com classificação. Pulando fold.\")\n",
        "                 return\n",
        "        else:\n",
        "            for k_try in range(2, max_possible_k + 1):\n",
        "                try:\n",
        "                    kmeans = KMeans(n_clusters=k_try, random_state=self.config['random_seed'], n_init='auto').fit(train_embeddings)\n",
        "                    if len(np.unique(kmeans.labels_)) > 1:\n",
        "                        score = silhouette_score(train_embeddings, kmeans.labels_)\n",
        "                        if score > best_silhouette_score:\n",
        "                            best_silhouette_score = score\n",
        "                            best_k = k_try\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro durante K-Means ou Silhouette para k={k_try}: {e}\")\n",
        "                    continue\n",
        "        print(f\"Fold {fold_idx + 1}: Número de classes estimado (best_k) = {best_k} (Silhouette: {best_silhouette_score:.3f})\")\n",
        "\n",
        "        final_kmeans = KMeans(n_clusters=best_k, random_state=self.config['random_seed'], n_init='auto').fit(train_embeddings)\n",
        "        train_pseudo_labels = final_kmeans.labels_\n",
        "\n",
        "        val_pseudo_labels = np.array([])\n",
        "        if len(val_embeddings) > 0:\n",
        "            try:\n",
        "                val_pseudo_labels = final_kmeans.predict(val_embeddings)\n",
        "            except Exception as e: # Kmeans pode não conseguir prever se val_embeddings for muito diferente\n",
        "                print(f\"Erro ao prever pseudo-rótulos de validação: {e}. Validação será limitada.\")\n",
        "        else:\n",
        "            print(f\"Fold {fold_idx + 1}: Nenhum embedding de validação. Validação será limitada.\")\n",
        "\n",
        "\n",
        "        # ETAPA 4: Instanciar e Treinar Modelos Finais com Pseudo-Rótulos\n",
        "        print(f\"Fold {fold_idx + 1}: (Etapa 4) Preparando modelos finais...\")\n",
        "        cnn_classifier = VibrationAnalysisCNN(num_classes=best_k).to(self.device)\n",
        "\n",
        "        simclr_lora_classifier = SimCLRModelLoRA(\n",
        "            projection_dim=self.config['projection_dim_simclr'],\n",
        "            num_classes=best_k,\n",
        "            lora_rank=self.config['lora_rank'],\n",
        "            lora_scale=self.config['lora_scale']\n",
        "        ).to(self.device)\n",
        "\n",
        "        if hasattr(encoder_trainer_model, 'encoder'): # Checar se o modelo de treino do encoder existe\n",
        "            simclr_lora_classifier.encoder.load_state_dict(encoder_trainer_model.encoder.state_dict())\n",
        "            print(f\"Fold {fold_idx + 1}: Pesos do encoder pré-treinado carregados no SimCLRModelLoRA final.\")\n",
        "\n",
        "            if self.config['freeze_encoder_after_load']:\n",
        "                for param in simclr_lora_classifier.encoder.parameters():\n",
        "                    param.requires_grad = False\n",
        "                print(f\"Fold {fold_idx + 1}: Encoder do SimCLRModelLoRA final congelado.\")\n",
        "\n",
        "        self.agent.update_models({\n",
        "            'VibrationCNN': cnn_classifier,\n",
        "            'SimCLR_LoRA_Classifier': simclr_lora_classifier\n",
        "        })\n",
        "\n",
        "        sup_train_dataset = PLDS(train_subset, train_pseudo_labels)\n",
        "\n",
        "        sup_val_dataset = None\n",
        "        if len(val_pseudo_labels) > 0 and len(val_subset) == len(val_pseudo_labels):\n",
        "            sup_val_dataset = PLDS(val_subset, val_pseudo_labels)\n",
        "        else:\n",
        "            print(f\"Fold {fold_idx + 1}: Conjunto de validação ou pseudo-rótulos de validação incompatíveis/vazios. Validação será limitada.\")\n",
        "\n",
        "\n",
        "        # Loop de Treinamento Supervisionado\n",
        "        for model_name, model_instance in self.agent.models.items():\n",
        "            print(f\"\\nFold {fold_idx + 1}: Treinando modelo '{model_name}'...\")\n",
        "            model_instance.to(self.device)\n",
        "\n",
        "            trainable_params = filter(lambda p: p.requires_grad, model_instance.parameters())\n",
        "            optimizer = torch.optim.Adam(trainable_params, lr=self.config['learning_rate_classifier'])\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "            history = {'epoch': [], 'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "            train_loader_sup = DataLoader(sup_train_dataset, batch_size=self.config['batch_size'], shuffle=True,\n",
        "                                          num_workers=self.config.get('num_workers', 0), pin_memory=True)\n",
        "\n",
        "            val_loader_sup = None\n",
        "            if sup_val_dataset and len(sup_val_dataset) > 0:\n",
        "                 val_loader_sup = DataLoader(sup_val_dataset, batch_size=self.config['batch_size'], shuffle=False,\n",
        "                                        num_workers=self.config.get('num_workers', 0), pin_memory=True)\n",
        "\n",
        "            for epoch in range(self.config['epochs_classifier']):\n",
        "                model_instance.train()\n",
        "                epoch_train_loss = 0\n",
        "                if len(train_loader_sup) == 0:\n",
        "                    print(f\"Epoch {epoch+1}/{self.config['epochs_classifier']} - {model_name} - Train loader vazio. Pulando treino.\")\n",
        "                    history['train_loss'].append(0)\n",
        "                    history['epoch'].append(epoch + 1)\n",
        "                    history['val_loss'].append(0)\n",
        "                    history['val_acc'].append(0)\n",
        "                    continue # Pula para a próxima época\n",
        "\n",
        "                for batch_idx, (specs, labels) in enumerate(train_loader_sup): # CORRIGIDO AQUI\n",
        "                    specs, labels = specs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    if model_name == 'VibrationCNN':\n",
        "                        outputs = model_instance(specs)\n",
        "                    else:\n",
        "                        outputs = model_instance.forward_classifier(specs)\n",
        "\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_train_loss += loss.item()\n",
        "\n",
        "                avg_epoch_train_loss = epoch_train_loss / len(train_loader_sup) if len(train_loader_sup) > 0 else 0\n",
        "                history['train_loss'].append(avg_epoch_train_loss)\n",
        "                history['epoch'].append(epoch + 1)\n",
        "\n",
        "                epoch_val_loss = 0\n",
        "                correct_val = 0\n",
        "                total_val = 0\n",
        "                all_preds_val, all_labels_val = [], []\n",
        "\n",
        "                if val_loader_sup and len(val_loader_sup) > 0:\n",
        "                    model_instance.eval()\n",
        "                    with torch.no_grad():\n",
        "                        for specs, labels in val_loader_sup: # CORRIGIDO AQUI\n",
        "                            specs, labels = specs.to(self.device), labels.to(self.device)\n",
        "                            if model_name == 'VibrationCNN':\n",
        "                                outputs = model_instance(specs)\n",
        "                            else:\n",
        "                                outputs = model_instance.forward_classifier(specs)\n",
        "\n",
        "                            loss_val = criterion(outputs, labels) # Renomear variável de loss\n",
        "                            epoch_val_loss += loss_val.item()\n",
        "\n",
        "                            _, predicted = torch.max(outputs.data, 1)\n",
        "                            total_val += labels.size(0)\n",
        "                            correct_val += (predicted == labels).sum().item()\n",
        "                            all_preds_val.extend(predicted.cpu().numpy())\n",
        "                            all_labels_val.extend(labels.cpu().numpy())\n",
        "\n",
        "                    avg_epoch_val_loss = epoch_val_loss / len(val_loader_sup) if len(val_loader_sup) > 0 else 0\n",
        "                    val_accuracy = correct_val / total_val if total_val > 0 else 0\n",
        "                    history['val_loss'].append(avg_epoch_val_loss)\n",
        "                    history['val_acc'].append(val_accuracy)\n",
        "                    print(f\"Epoch {epoch+1}/{self.config['epochs_classifier']} - {model_name} - Train Loss: {avg_epoch_train_loss:.4f}, Val Loss: {avg_epoch_val_loss:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "                else:\n",
        "                    history['val_loss'].append(0)\n",
        "                    history['val_acc'].append(0)\n",
        "                    print(f\"Epoch {epoch+1}/{self.config['epochs_classifier']} - {model_name} - Train Loss: {avg_epoch_train_loss:.4f} (Validação pulada ou loader vazio)\")\n",
        "\n",
        "            if val_loader_sup and total_val > 0 :\n",
        "                metrics = {\n",
        "                    'accuracy': accuracy_score(all_labels_val, all_preds_val),\n",
        "                    'precision': precision_score(all_labels_val, all_preds_val, average='weighted', zero_division=0),\n",
        "                    'recall': recall_score(all_labels_val, all_preds_val, average='weighted', zero_division=0),\n",
        "                    'f1': f1_score(all_labels_val, all_preds_val, average='weighted', zero_division=0)\n",
        "                }\n",
        "                print(f\"Fold {fold_idx + 1} - Modelo '{model_name}' - Métricas Finais (Val): \"\n",
        "                      f\"Acc: {metrics['accuracy']:.3f}, Prec: {metrics['precision']:.3f}, \"\n",
        "                      f\"Rec: {metrics['recall']:.3f}, F1: {metrics['f1']:.3f}\")\n",
        "                self.agent.update_performance(model_name, metrics['f1'])\n",
        "\n",
        "                metrics_df = pd.DataFrame([metrics])\n",
        "                metrics_df.to_csv(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_metrics.csv'), index=False)\n",
        "\n",
        "                cm = confusion_matrix(all_labels_val, all_preds_val, labels=list(range(best_k))) # Adicionar labels para garantir o tamanho da matriz\n",
        "                plt.figure(figsize=(max(6, best_k), max(5, best_k-1)))\n",
        "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=range(best_k), yticklabels=range(best_k))\n",
        "                plt.title(f'Matriz de Confusão - Fold {fold_idx+1} - {model_name}')\n",
        "                plt.xlabel('Predito'); plt.ylabel('Real')\n",
        "                plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_cm.png'))\n",
        "                plt.close()\n",
        "            else:\n",
        "                print(f\"Fold {fold_idx + 1} - Modelo '{model_name}' - Validação pulada ou sem dados válidos para métricas.\")\n",
        "                self.agent.update_performance(model_name, 0.0)\n",
        "\n",
        "            history_df = pd.DataFrame(history)\n",
        "            history_df.to_csv(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_history.csv'), index=False)\n",
        "\n",
        "            plt.figure()\n",
        "            plt.plot(history_df['epoch'], history_df['train_loss'], label='Train Loss')\n",
        "            if val_loader_sup and len(val_loader_sup) > 0 : plt.plot(history_df['epoch'], history_df['val_loss'], label='Val Loss')\n",
        "            plt.legend(); plt.title(f'Curva de Loss - Fold {fold_idx+1} - {model_name}')\n",
        "            plt.xlabel('Epoch'); plt.ylabel('Loss')\n",
        "            plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_loss_curve.png'))\n",
        "            plt.close()\n",
        "\n",
        "            if val_loader_sup and len(val_loader_sup) > 0:\n",
        "                plt.figure()\n",
        "                plt.plot(history_df['epoch'], history_df['val_acc'], label='Val Accuracy')\n",
        "                plt.legend(); plt.title(f'Curva de Acurácia (Val) - Fold {fold_idx+1} - {model_name}')\n",
        "                plt.xlabel('Epoch'); plt.ylabel('Accuracy')\n",
        "                plt.savefig(os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_acc_curve.png'))\n",
        "                plt.close()\n",
        "\n",
        "            torch.save(model_instance.state_dict(), os.path.join(self.config['output_dir'], f'fold{fold_idx+1}_{model_name}_model.pt'))\n",
        "            print(f\"Modelo '{model_name}' salvo para o Fold {fold_idx+1}.\")\n",
        "\n",
        "\n",
        "    def cross_validate(self, full_dataset: UnsupervisedAeroDataset):\n",
        "        if len(full_dataset) < self.config.get('n_folds', 1) : # n_folds deve ser pelo menos 1\n",
        "            print(f\"Erro: Número de amostras ({len(full_dataset)}) é menor que o número mínimo de folds requerido.\")\n",
        "            print(\"Reduza n_folds ou forneça mais dados.\")\n",
        "            return\n",
        "\n",
        "        kf = KFold(n_splits=self.config['n_folds'], shuffle=True, random_state=self.config['random_seed'])\n",
        "\n",
        "        for fold_index, (train_indices, val_indices) in enumerate(kf.split(range(len(full_dataset)))): # Usar range(len()) para kf.split\n",
        "            if len(train_indices) == 0 or len(val_indices) == 0:\n",
        "                print(f\"Fold {fold_index + 1} tem 0 amostras de treino ou validação. Pulando este fold.\")\n",
        "                continue\n",
        "            train_subset = Subset(full_dataset, train_indices)\n",
        "            val_subset = Subset(full_dataset, val_indices)\n",
        "            self.train_and_eval_fold(train_subset, val_subset, fold_index)\n",
        "\n",
        "        print(\"\\n--- Validação Cruzada Concluída ---\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    config = {\n",
        "        'csv_paths': [\n",
        "          '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET06.csv',\n",
        "          '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET07.csv'\n",
        "        ],\n",
        "        'window_size': 1024, 'overlap': 0.5, 'sampling_rate': 12800,\n",
        "        'output_dir': '/content/drive/My Drive/aero_results/',\n",
        "        'random_seed': 42,\n",
        "        'num_workers': 0, # Para Colab, 0 ou 2. Comece com 0 se houver problemas com workers.\n",
        "\n",
        "        'projection_dim_simclr': 128,\n",
        "        'simclr_epochs_for_kmeans': 5,\n",
        "        'learning_rate_simclr_kmeans': 1e-4,\n",
        "        'max_k_means_clusters': 10,\n",
        "\n",
        "        'learning_rate_classifier': 1e-4,\n",
        "        'epochs_classifier': 30,\n",
        "        'batch_size': 32,\n",
        "\n",
        "        'lora_rank': 8, 'lora_scale': 0.01,\n",
        "        'freeze_encoder_after_load': False,\n",
        "\n",
        "        'n_folds': 5\n",
        "    }\n",
        "\n",
        "    np.random.seed(config['random_seed'])\n",
        "    torch.manual_seed(config['random_seed'])\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(config['random_seed'])\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    if not config['csv_paths']:\n",
        "        print(\"Nenhum caminho CSV fornecido. Criando dummy CSVs para teste.\")\n",
        "        ensure_dir('dummy_data_final')\n",
        "        for i in range(2):\n",
        "            dummy_signal_length = config['window_size'] * 2\n",
        "            data = np.random.rand(100, 2 + dummy_signal_length)\n",
        "            pd.DataFrame(data).to_csv(f'dummy_data_final/dummy_dataset_{i+1}.csv', header=False, index=False)\n",
        "            config['csv_paths'].append(f'dummy_data_final/dummy_dataset_{i+1}.csv')\n",
        "\n",
        "    signal_preprocessor = SignalPreprocessor(\n",
        "        window_size=config['window_size'],\n",
        "        overlap=config['overlap'],\n",
        "        sampling_rate=config['sampling_rate']\n",
        "    )\n",
        "\n",
        "    data_transform = transforms.Compose([\n",
        "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        full_aero_dataset = UnsupervisedAeroDataset(\n",
        "            csv_paths=config['csv_paths'],\n",
        "            signal_preprocessor=signal_preprocessor,\n",
        "            transform=data_transform\n",
        "        )\n",
        "    except ValueError as e:\n",
        "        print(f\"Erro ao inicializar o dataset: {e}\")\n",
        "        print(\"Encerrando o script.\")\n",
        "        exit()\n",
        "\n",
        "    if len(full_aero_dataset) == 0:\n",
        "        print(\"Dataset está vazio após o processamento. Verifique os dados de entrada e os logs.\")\n",
        "        print(\"Encerrando o script.\")\n",
        "        exit()\n",
        "\n",
        "    print(f\"Dataset carregado com {len(full_aero_dataset)} amostras.\")\n",
        "\n",
        "    model_agent = ModelAgent()\n",
        "    vibration_analyzer = VibrationAnalyzer(config, model_agent, signal_preprocessor)\n",
        "\n",
        "    vibration_analyzer.cross_validate(full_aero_dataset)\n",
        "\n",
        "    print(\"\\nAnálise concluída.\")\n",
        "    if model_agent.models:\n",
        "        print(\"Scores F1 finais (do último fold processado com sucesso):\")\n",
        "        for name, score in model_agent.scores.items():\n",
        "            print(f\"  {name}: {score:.4f}\")\n",
        "        best_model_overall = model_agent.get_best_model_name()\n",
        "        if best_model_overall:\n",
        "            print(f\"Melhor modelo no último fold (baseado em F1): {best_model_overall} \"\n",
        "                  f\"(F1: {model_agent.scores[best_model_overall]:.4f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLuHxw-k8YpL",
        "outputId": "a010f01f-4da3-4fb9-c5cb-fe79d35750cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando e pré-processando dados de: ['/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET06.csv', '/content/drive/My Drive/DATASET_AERO_SWEDEN/DATASET07.csv']\n",
            "Total de 5929 amostras carregadas.\n",
            "Dataset carregado com 5929 amostras.\n",
            "Usando dispositivo: cuda\n",
            "\n",
            "--- Fold 1 ---\n",
            "Fold 1: (Etapa 1) Pré-treinando encoder para K-Means...\n",
            "Iniciando pré-treinamento do encoder por 5 épocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0798\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0040\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0029\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0028\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0025\n",
            "Pré-treinamento do encoder concluído.\n",
            "Fold 1: (Etapa 2) Extraindo embeddings...\n",
            "Fold 1: (Etapa 3) Executando K-Means...\n",
            "Fold 1: Número de classes estimado (best_k) = 2 (Silhouette: 0.232)\n",
            "Fold 1: (Etapa 4) Preparando modelos finais...\n",
            "Fold 1: Pesos do encoder pré-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 1: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6913, Val Loss: 0.6906, Val Acc: 0.5253\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6892, Val Loss: 0.6896, Val Acc: 0.5253\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6898, Val Loss: 0.6888, Val Acc: 0.5253\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6886, Val Loss: 0.6881, Val Acc: 0.5337\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6884, Val Loss: 0.6881, Val Acc: 0.5253\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6888, Val Loss: 0.6872, Val Acc: 0.5388\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6882, Val Loss: 0.6862, Val Acc: 0.5388\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6864, Val Loss: 0.6858, Val Acc: 0.5430\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6853, Val Loss: 0.6864, Val Acc: 0.5253\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6838, Val Loss: 0.6802, Val Acc: 0.5540\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.6781, Val Loss: 0.6740, Val Acc: 0.5641\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.6714, Val Loss: 0.6555, Val Acc: 0.6231\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.6494, Val Loss: 0.6341, Val Acc: 0.6062\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.6279, Val Loss: 0.6173, Val Acc: 0.6703\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.5870, Val Loss: 0.6090, Val Acc: 0.6315\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.5641, Val Loss: 0.5358, Val Acc: 0.7782\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.5332, Val Loss: 0.5250, Val Acc: 0.7437\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.5016, Val Loss: 0.5738, Val Acc: 0.6771\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.5284, Val Loss: 0.4848, Val Acc: 0.7884\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.4980, Val Loss: 0.4808, Val Acc: 0.7884\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.4722, Val Loss: 0.4981, Val Acc: 0.7639\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.4845, Val Loss: 0.5872, Val Acc: 0.7015\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.4632, Val Loss: 0.4778, Val Acc: 0.7850\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.4677, Val Loss: 0.4601, Val Acc: 0.7909\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4471, Val Loss: 0.4485, Val Acc: 0.8010\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.4442, Val Loss: 0.4481, Val Acc: 0.7968\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4648, Val Loss: 0.4438, Val Acc: 0.7993\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.4421, Val Loss: 0.5335, Val Acc: 0.7496\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.4448, Val Loss: 0.4676, Val Acc: 0.7892\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.4465, Val Loss: 0.4452, Val Acc: 0.8002\n",
            "Fold 1 - Modelo 'VibrationCNN' - Métricas Finais (Val): Acc: 0.800, Prec: 0.803, Rec: 0.800, F1: 0.799\n",
            "Modelo 'VibrationCNN' salvo para o Fold 1.\n",
            "\n",
            "Fold 1: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6899, Val Loss: 0.6919, Val Acc: 0.5253\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6901, Val Loss: 0.6896, Val Acc: 0.5253\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6889, Val Loss: 0.6891, Val Acc: 0.5253\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6890, Val Loss: 0.6893, Val Acc: 0.5253\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6892, Val Loss: 0.6881, Val Acc: 0.5253\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6889, Val Loss: 0.6878, Val Acc: 0.5270\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6892, Val Loss: 0.6878, Val Acc: 0.5270\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6880, Val Loss: 0.6873, Val Acc: 0.5312\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6882, Val Loss: 0.6868, Val Acc: 0.5363\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6878, Val Loss: 0.6868, Val Acc: 0.5582\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6880, Val Loss: 0.6855, Val Acc: 0.5388\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6861, Val Loss: 0.6842, Val Acc: 0.5388\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6858, Val Loss: 0.6872, Val Acc: 0.5253\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6830, Val Loss: 0.6827, Val Acc: 0.5295\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6831, Val Loss: 0.6793, Val Acc: 0.5413\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6807, Val Loss: 0.6756, Val Acc: 0.6223\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6775, Val Loss: 0.6710, Val Acc: 0.5691\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6729, Val Loss: 0.6721, Val Acc: 0.5278\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6697, Val Loss: 0.6622, Val Acc: 0.6020\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6651, Val Loss: 0.6592, Val Acc: 0.5632\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6575, Val Loss: 0.6478, Val Acc: 0.6610\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6516, Val Loss: 0.6456, Val Acc: 0.7201\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6461, Val Loss: 0.6362, Val Acc: 0.6383\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6387, Val Loss: 0.6225, Val Acc: 0.7091\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6306, Val Loss: 0.6155, Val Acc: 0.7352\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6227, Val Loss: 0.6199, Val Acc: 0.6324\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6153, Val Loss: 0.5974, Val Acc: 0.7403\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6061, Val Loss: 0.5908, Val Acc: 0.7234\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6054, Val Loss: 0.5807, Val Acc: 0.7462\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.5893, Val Loss: 0.5709, Val Acc: 0.7656\n",
            "Fold 1 - Modelo 'SimCLR_LoRA_Classifier' - Métricas Finais (Val): Acc: 0.766, Prec: 0.766, Rec: 0.766, F1: 0.766\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 1.\n",
            "\n",
            "--- Fold 2 ---\n",
            "Fold 2: (Etapa 1) Pré-treinando encoder para K-Means...\n",
            "Iniciando pré-treinamento do encoder por 5 épocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0772\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0054\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0033\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0027\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0025\n",
            "Pré-treinamento do encoder concluído.\n",
            "Fold 2: (Etapa 2) Extraindo embeddings...\n",
            "Fold 2: (Etapa 3) Executando K-Means...\n",
            "Fold 2: Número de classes estimado (best_k) = 2 (Silhouette: 0.208)\n",
            "Fold 2: (Etapa 4) Preparando modelos finais...\n",
            "Fold 2: Pesos do encoder pré-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 2: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6766, Val Loss: 0.6756, Val Acc: 0.5843\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6747, Val Loss: 0.6738, Val Acc: 0.5843\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6734, Val Loss: 0.6746, Val Acc: 0.5843\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6744, Val Loss: 0.6727, Val Acc: 0.5843\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6730, Val Loss: 0.6736, Val Acc: 0.5843\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6705, Val Loss: 0.6711, Val Acc: 0.5843\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6691, Val Loss: 0.6675, Val Acc: 0.5843\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6631, Val Loss: 0.6614, Val Acc: 0.5843\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6486, Val Loss: 0.6363, Val Acc: 0.6113\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6286, Val Loss: 0.6137, Val Acc: 0.6113\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.5968, Val Loss: 0.5830, Val Acc: 0.7243\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.5626, Val Loss: 0.5517, Val Acc: 0.7243\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.5367, Val Loss: 0.5291, Val Acc: 0.7639\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.5159, Val Loss: 0.4952, Val Acc: 0.7732\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.5021, Val Loss: 0.4811, Val Acc: 0.7901\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.4722, Val Loss: 0.5075, Val Acc: 0.7589\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.4640, Val Loss: 0.4817, Val Acc: 0.7774\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.4489, Val Loss: 0.4744, Val Acc: 0.7757\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.4327, Val Loss: 0.4259, Val Acc: 0.8052\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.4292, Val Loss: 0.4152, Val Acc: 0.8212\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.4358, Val Loss: 0.4549, Val Acc: 0.7884\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.4181, Val Loss: 0.4899, Val Acc: 0.7664\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.4097, Val Loss: 0.4543, Val Acc: 0.7901\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.3973, Val Loss: 0.4888, Val Acc: 0.7614\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4066, Val Loss: 0.3885, Val Acc: 0.8297\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.3955, Val Loss: 0.3998, Val Acc: 0.8145\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4002, Val Loss: 0.4224, Val Acc: 0.8061\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.3864, Val Loss: 0.4099, Val Acc: 0.8069\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.3807, Val Loss: 0.5080, Val Acc: 0.7589\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.3987, Val Loss: 0.3638, Val Acc: 0.8474\n",
            "Fold 2 - Modelo 'VibrationCNN' - Métricas Finais (Val): Acc: 0.847, Prec: 0.847, Rec: 0.847, F1: 0.846\n",
            "Modelo 'VibrationCNN' salvo para o Fold 2.\n",
            "\n",
            "Fold 2: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6779, Val Loss: 0.6741, Val Acc: 0.5843\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6763, Val Loss: 0.6740, Val Acc: 0.5843\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6735, Val Loss: 0.6736, Val Acc: 0.5843\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6756, Val Loss: 0.6733, Val Acc: 0.5843\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6746, Val Loss: 0.6733, Val Acc: 0.5843\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6730, Val Loss: 0.6733, Val Acc: 0.5843\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6725, Val Loss: 0.6723, Val Acc: 0.5843\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6719, Val Loss: 0.6720, Val Acc: 0.5843\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6704, Val Loss: 0.6719, Val Acc: 0.5843\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6681, Val Loss: 0.6693, Val Acc: 0.5843\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6689, Val Loss: 0.6676, Val Acc: 0.5843\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6658, Val Loss: 0.6666, Val Acc: 0.5860\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6633, Val Loss: 0.6686, Val Acc: 0.5936\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6621, Val Loss: 0.6634, Val Acc: 0.5911\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6590, Val Loss: 0.6604, Val Acc: 0.5911\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6565, Val Loss: 0.6563, Val Acc: 0.5944\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6533, Val Loss: 0.6562, Val Acc: 0.5902\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6490, Val Loss: 0.6496, Val Acc: 0.5944\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6441, Val Loss: 0.6447, Val Acc: 0.5970\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6389, Val Loss: 0.6409, Val Acc: 0.6400\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6351, Val Loss: 0.6337, Val Acc: 0.6400\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6313, Val Loss: 0.6285, Val Acc: 0.6577\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6268, Val Loss: 0.6279, Val Acc: 0.6914\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6196, Val Loss: 0.6159, Val Acc: 0.6509\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6149, Val Loss: 0.6082, Val Acc: 0.6644\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6110, Val Loss: 0.6099, Val Acc: 0.6484\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6063, Val Loss: 0.6057, Val Acc: 0.7218\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.5922, Val Loss: 0.6076, Val Acc: 0.6046\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.5920, Val Loss: 0.5821, Val Acc: 0.7049\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.5821, Val Loss: 0.5874, Val Acc: 0.6374\n",
            "Fold 2 - Modelo 'SimCLR_LoRA_Classifier' - Métricas Finais (Val): Acc: 0.637, Prec: 0.711, Rec: 0.637, F1: 0.554\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 2.\n",
            "\n",
            "--- Fold 3 ---\n",
            "Fold 3: (Etapa 1) Pré-treinando encoder para K-Means...\n",
            "Iniciando pré-treinamento do encoder por 5 épocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0624\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0029\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0023\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0020\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0020\n",
            "Pré-treinamento do encoder concluído.\n",
            "Fold 3: (Etapa 2) Extraindo embeddings...\n",
            "Fold 3: (Etapa 3) Executando K-Means...\n",
            "Fold 3: Número de classes estimado (best_k) = 2 (Silhouette: 0.206)\n",
            "Fold 3: (Etapa 4) Preparando modelos finais...\n",
            "Fold 3: Pesos do encoder pré-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 3: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6765, Val Loss: 0.6739, Val Acc: 0.5911\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6708, Val Loss: 0.6680, Val Acc: 0.5911\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6685, Val Loss: 0.6661, Val Acc: 0.5911\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6681, Val Loss: 0.6647, Val Acc: 0.6003\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6679, Val Loss: 0.6632, Val Acc: 0.5927\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6634, Val Loss: 0.6636, Val Acc: 0.6223\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6614, Val Loss: 0.6580, Val Acc: 0.6214\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6593, Val Loss: 0.6537, Val Acc: 0.6526\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6552, Val Loss: 0.6508, Val Acc: 0.6172\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6510, Val Loss: 0.6399, Val Acc: 0.6728\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.6466, Val Loss: 0.6315, Val Acc: 0.6686\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.6372, Val Loss: 0.6223, Val Acc: 0.6459\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.6126, Val Loss: 0.6060, Val Acc: 0.7234\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.5904, Val Loss: 0.5692, Val Acc: 0.7083\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.5857, Val Loss: 0.5646, Val Acc: 0.7074\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.5608, Val Loss: 0.5975, Val Acc: 0.6509\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.5525, Val Loss: 0.5207, Val Acc: 0.7504\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.5268, Val Loss: 0.5526, Val Acc: 0.7150\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.5116, Val Loss: 0.5317, Val Acc: 0.7209\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.5025, Val Loss: 0.4825, Val Acc: 0.7774\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.5088, Val Loss: 0.5325, Val Acc: 0.7201\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.4839, Val Loss: 0.4763, Val Acc: 0.7690\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.4796, Val Loss: 0.5218, Val Acc: 0.7378\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.4756, Val Loss: 0.4815, Val Acc: 0.7673\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4760, Val Loss: 0.4688, Val Acc: 0.7681\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.4633, Val Loss: 0.4548, Val Acc: 0.7766\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4974, Val Loss: 0.4534, Val Acc: 0.7808\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.4804, Val Loss: 0.4569, Val Acc: 0.7707\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.4582, Val Loss: 0.4464, Val Acc: 0.7850\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.4540, Val Loss: 0.4717, Val Acc: 0.7673\n",
            "Fold 3 - Modelo 'VibrationCNN' - Métricas Finais (Val): Acc: 0.767, Prec: 0.783, Rec: 0.767, F1: 0.754\n",
            "Modelo 'VibrationCNN' salvo para o Fold 3.\n",
            "\n",
            "Fold 3: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6787, Val Loss: 0.6737, Val Acc: 0.5911\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6722, Val Loss: 0.6701, Val Acc: 0.5911\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6706, Val Loss: 0.6687, Val Acc: 0.5911\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6686, Val Loss: 0.6675, Val Acc: 0.5911\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6658, Val Loss: 0.6660, Val Acc: 0.5919\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6664, Val Loss: 0.6658, Val Acc: 0.5919\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6680, Val Loss: 0.6654, Val Acc: 0.5902\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6682, Val Loss: 0.6647, Val Acc: 0.5919\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6670, Val Loss: 0.6645, Val Acc: 0.5919\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6657, Val Loss: 0.6638, Val Acc: 0.5944\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6658, Val Loss: 0.6630, Val Acc: 0.5953\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6651, Val Loss: 0.6622, Val Acc: 0.6206\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6643, Val Loss: 0.6623, Val Acc: 0.6189\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6640, Val Loss: 0.6613, Val Acc: 0.5927\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6633, Val Loss: 0.6591, Val Acc: 0.6239\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6622, Val Loss: 0.6600, Val Acc: 0.6298\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6608, Val Loss: 0.6574, Val Acc: 0.5936\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6600, Val Loss: 0.6561, Val Acc: 0.6105\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6595, Val Loss: 0.6543, Val Acc: 0.5978\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6577, Val Loss: 0.6528, Val Acc: 0.6214\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6557, Val Loss: 0.6509, Val Acc: 0.6189\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6530, Val Loss: 0.6497, Val Acc: 0.6054\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6507, Val Loss: 0.6535, Val Acc: 0.5919\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6486, Val Loss: 0.6439, Val Acc: 0.6214\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6470, Val Loss: 0.6406, Val Acc: 0.6535\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6432, Val Loss: 0.6371, Val Acc: 0.6661\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6415, Val Loss: 0.6348, Val Acc: 0.6627\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6373, Val Loss: 0.6323, Val Acc: 0.6948\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6326, Val Loss: 0.6293, Val Acc: 0.6535\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6277, Val Loss: 0.6261, Val Acc: 0.6492\n",
            "Fold 3 - Modelo 'SimCLR_LoRA_Classifier' - Métricas Finais (Val): Acc: 0.649, Prec: 0.750, Rec: 0.649, F1: 0.563\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 3.\n",
            "\n",
            "--- Fold 4 ---\n",
            "Fold 4: (Etapa 1) Pré-treinando encoder para K-Means...\n",
            "Iniciando pré-treinamento do encoder por 5 épocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0887\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0048\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0031\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0027\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0025\n",
            "Pré-treinamento do encoder concluído.\n",
            "Fold 4: (Etapa 2) Extraindo embeddings...\n",
            "Fold 4: (Etapa 3) Executando K-Means...\n",
            "Fold 4: Número de classes estimado (best_k) = 2 (Silhouette: 0.219)\n",
            "Fold 4: (Etapa 4) Preparando modelos finais...\n",
            "Fold 4: Pesos do encoder pré-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 4: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6888, Val Loss: 0.6867, Val Acc: 0.5346\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6793, Val Loss: 0.6858, Val Acc: 0.5430\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6799, Val Loss: 0.6847, Val Acc: 0.5388\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6776, Val Loss: 0.6975, Val Acc: 0.5641\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6780, Val Loss: 0.6833, Val Acc: 0.5531\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6788, Val Loss: 0.6848, Val Acc: 0.5691\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6775, Val Loss: 0.6815, Val Acc: 0.5481\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6741, Val Loss: 0.6801, Val Acc: 0.5472\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6740, Val Loss: 0.6780, Val Acc: 0.5548\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6719, Val Loss: 0.6780, Val Acc: 0.5784\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.6701, Val Loss: 0.6703, Val Acc: 0.5691\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.6655, Val Loss: 0.6772, Val Acc: 0.5961\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.6607, Val Loss: 0.6636, Val Acc: 0.5894\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.6567, Val Loss: 0.6438, Val Acc: 0.6239\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.6476, Val Loss: 0.6511, Val Acc: 0.6265\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.6335, Val Loss: 0.6083, Val Acc: 0.6543\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.6006, Val Loss: 0.6699, Val Acc: 0.5472\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.6138, Val Loss: 0.5594, Val Acc: 0.7192\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.5631, Val Loss: 0.8116, Val Acc: 0.5236\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.5576, Val Loss: 0.4959, Val Acc: 0.7715\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.5286, Val Loss: 0.4773, Val Acc: 0.7833\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.5246, Val Loss: 0.5428, Val Acc: 0.6897\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.5099, Val Loss: 0.5036, Val Acc: 0.7243\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.5091, Val Loss: 0.4503, Val Acc: 0.7909\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4887, Val Loss: 0.4575, Val Acc: 0.7858\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.4901, Val Loss: 0.4495, Val Acc: 0.7934\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4793, Val Loss: 0.4588, Val Acc: 0.7841\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.4836, Val Loss: 0.4217, Val Acc: 0.8103\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.4705, Val Loss: 0.4474, Val Acc: 0.7723\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.4697, Val Loss: 0.4135, Val Acc: 0.7917\n",
            "Fold 4 - Modelo 'VibrationCNN' - Métricas Finais (Val): Acc: 0.792, Prec: 0.799, Rec: 0.792, F1: 0.790\n",
            "Modelo 'VibrationCNN' salvo para o Fold 4.\n",
            "\n",
            "Fold 4: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6916, Val Loss: 0.6893, Val Acc: 0.5472\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6842, Val Loss: 0.6859, Val Acc: 0.5405\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6813, Val Loss: 0.6844, Val Acc: 0.5371\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6791, Val Loss: 0.6839, Val Acc: 0.5481\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6771, Val Loss: 0.6829, Val Acc: 0.5430\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6777, Val Loss: 0.6818, Val Acc: 0.5430\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6756, Val Loss: 0.6816, Val Acc: 0.5497\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6744, Val Loss: 0.6803, Val Acc: 0.5531\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6757, Val Loss: 0.6790, Val Acc: 0.5531\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6732, Val Loss: 0.6820, Val Acc: 0.5632\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6723, Val Loss: 0.6773, Val Acc: 0.5548\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6720, Val Loss: 0.6766, Val Acc: 0.5616\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6706, Val Loss: 0.6751, Val Acc: 0.5624\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6690, Val Loss: 0.6799, Val Acc: 0.5818\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6664, Val Loss: 0.6709, Val Acc: 0.5978\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6652, Val Loss: 0.6710, Val Acc: 0.5978\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6650, Val Loss: 0.6684, Val Acc: 0.5700\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6635, Val Loss: 0.6674, Val Acc: 0.5750\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6595, Val Loss: 0.6655, Val Acc: 0.6096\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6597, Val Loss: 0.6582, Val Acc: 0.6164\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6532, Val Loss: 0.6537, Val Acc: 0.6180\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6519, Val Loss: 0.6665, Val Acc: 0.5826\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6486, Val Loss: 0.6423, Val Acc: 0.6594\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6431, Val Loss: 0.6500, Val Acc: 0.6164\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6400, Val Loss: 0.6356, Val Acc: 0.6745\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6335, Val Loss: 0.6339, Val Acc: 0.6484\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6331, Val Loss: 0.6231, Val Acc: 0.6821\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6257, Val Loss: 0.6173, Val Acc: 0.7007\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6230, Val Loss: 0.6179, Val Acc: 0.6771\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6168, Val Loss: 0.6216, Val Acc: 0.6543\n",
            "Fold 4 - Modelo 'SimCLR_LoRA_Classifier' - Métricas Finais (Val): Acc: 0.654, Prec: 0.677, Rec: 0.654, F1: 0.642\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 4.\n",
            "\n",
            "--- Fold 5 ---\n",
            "Fold 5: (Etapa 1) Pré-treinando encoder para K-Means...\n",
            "Iniciando pré-treinamento do encoder por 5 épocas...\n",
            "Encoder Pretrain Epoch 1/5, Loss: 0.0703\n",
            "Encoder Pretrain Epoch 2/5, Loss: 0.0036\n",
            "Encoder Pretrain Epoch 3/5, Loss: 0.0027\n",
            "Encoder Pretrain Epoch 4/5, Loss: 0.0023\n",
            "Encoder Pretrain Epoch 5/5, Loss: 0.0020\n",
            "Pré-treinamento do encoder concluído.\n",
            "Fold 5: (Etapa 2) Extraindo embeddings...\n",
            "Fold 5: (Etapa 3) Executando K-Means...\n",
            "Fold 5: Número de classes estimado (best_k) = 2 (Silhouette: 0.167)\n",
            "Fold 5: (Etapa 4) Preparando modelos finais...\n",
            "Fold 5: Pesos do encoder pré-treinado carregados no SimCLRModelLoRA final.\n",
            "\n",
            "Fold 5: Treinando modelo 'VibrationCNN'...\n",
            "Epoch 1/30 - VibrationCNN - Train Loss: 0.6933, Val Loss: 0.6941, Val Acc: 0.4895\n",
            "Epoch 2/30 - VibrationCNN - Train Loss: 0.6931, Val Loss: 0.6928, Val Acc: 0.5232\n",
            "Epoch 3/30 - VibrationCNN - Train Loss: 0.6934, Val Loss: 0.6936, Val Acc: 0.5030\n",
            "Epoch 4/30 - VibrationCNN - Train Loss: 0.6925, Val Loss: 0.6928, Val Acc: 0.5215\n",
            "Epoch 5/30 - VibrationCNN - Train Loss: 0.6929, Val Loss: 0.6955, Val Acc: 0.4895\n",
            "Epoch 6/30 - VibrationCNN - Train Loss: 0.6925, Val Loss: 0.6921, Val Acc: 0.5232\n",
            "Epoch 7/30 - VibrationCNN - Train Loss: 0.6920, Val Loss: 0.6938, Val Acc: 0.5131\n",
            "Epoch 8/30 - VibrationCNN - Train Loss: 0.6914, Val Loss: 0.6931, Val Acc: 0.5148\n",
            "Epoch 9/30 - VibrationCNN - Train Loss: 0.6898, Val Loss: 0.6884, Val Acc: 0.5536\n",
            "Epoch 10/30 - VibrationCNN - Train Loss: 0.6873, Val Loss: 0.6836, Val Acc: 0.6000\n",
            "Epoch 11/30 - VibrationCNN - Train Loss: 0.6792, Val Loss: 0.6837, Val Acc: 0.5392\n",
            "Epoch 12/30 - VibrationCNN - Train Loss: 0.6671, Val Loss: 0.6941, Val Acc: 0.4928\n",
            "Epoch 13/30 - VibrationCNN - Train Loss: 0.6506, Val Loss: 0.6316, Val Acc: 0.7156\n",
            "Epoch 14/30 - VibrationCNN - Train Loss: 0.6305, Val Loss: 0.6173, Val Acc: 0.6616\n",
            "Epoch 15/30 - VibrationCNN - Train Loss: 0.5917, Val Loss: 0.5640, Val Acc: 0.7502\n",
            "Epoch 16/30 - VibrationCNN - Train Loss: 0.5647, Val Loss: 0.5688, Val Acc: 0.7173\n",
            "Epoch 17/30 - VibrationCNN - Train Loss: 0.5557, Val Loss: 0.5288, Val Acc: 0.7629\n",
            "Epoch 18/30 - VibrationCNN - Train Loss: 0.5287, Val Loss: 0.5467, Val Acc: 0.7215\n",
            "Epoch 19/30 - VibrationCNN - Train Loss: 0.5159, Val Loss: 0.5386, Val Acc: 0.7114\n",
            "Epoch 20/30 - VibrationCNN - Train Loss: 0.4974, Val Loss: 0.5185, Val Acc: 0.7367\n",
            "Epoch 21/30 - VibrationCNN - Train Loss: 0.4969, Val Loss: 0.4947, Val Acc: 0.7637\n",
            "Epoch 22/30 - VibrationCNN - Train Loss: 0.4902, Val Loss: 0.7772, Val Acc: 0.6008\n",
            "Epoch 23/30 - VibrationCNN - Train Loss: 0.4860, Val Loss: 0.4959, Val Acc: 0.7637\n",
            "Epoch 24/30 - VibrationCNN - Train Loss: 0.4702, Val Loss: 0.5456, Val Acc: 0.7046\n",
            "Epoch 25/30 - VibrationCNN - Train Loss: 0.4587, Val Loss: 0.4623, Val Acc: 0.7831\n",
            "Epoch 26/30 - VibrationCNN - Train Loss: 0.4548, Val Loss: 0.4569, Val Acc: 0.7848\n",
            "Epoch 27/30 - VibrationCNN - Train Loss: 0.4660, Val Loss: 0.4997, Val Acc: 0.7460\n",
            "Epoch 28/30 - VibrationCNN - Train Loss: 0.4517, Val Loss: 0.5092, Val Acc: 0.7485\n",
            "Epoch 29/30 - VibrationCNN - Train Loss: 0.4386, Val Loss: 0.4487, Val Acc: 0.7882\n",
            "Epoch 30/30 - VibrationCNN - Train Loss: 0.4463, Val Loss: 0.4807, Val Acc: 0.7662\n",
            "Fold 5 - Modelo 'VibrationCNN' - Métricas Finais (Val): Acc: 0.766, Prec: 0.785, Rec: 0.766, F1: 0.763\n",
            "Modelo 'VibrationCNN' salvo para o Fold 5.\n",
            "\n",
            "Fold 5: Treinando modelo 'SimCLR_LoRA_Classifier'...\n",
            "Epoch 1/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6935, Val Loss: 0.6938, Val Acc: 0.4895\n",
            "Epoch 2/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6931, Val Loss: 0.6933, Val Acc: 0.5063\n",
            "Epoch 3/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6930, Val Loss: 0.6934, Val Acc: 0.4970\n",
            "Epoch 4/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6929, Val Loss: 0.6933, Val Acc: 0.5072\n",
            "Epoch 5/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6922, Val Loss: 0.6923, Val Acc: 0.5181\n",
            "Epoch 6/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6921, Val Loss: 0.6915, Val Acc: 0.5274\n",
            "Epoch 7/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6916, Val Loss: 0.6919, Val Acc: 0.5392\n",
            "Epoch 8/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6906, Val Loss: 0.6925, Val Acc: 0.5257\n",
            "Epoch 9/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6907, Val Loss: 0.6901, Val Acc: 0.5468\n",
            "Epoch 10/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6891, Val Loss: 0.6885, Val Acc: 0.5603\n",
            "Epoch 11/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6879, Val Loss: 0.6879, Val Acc: 0.5586\n",
            "Epoch 12/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6859, Val Loss: 0.6893, Val Acc: 0.5359\n",
            "Epoch 13/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6846, Val Loss: 0.6834, Val Acc: 0.5477\n",
            "Epoch 14/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6819, Val Loss: 0.6795, Val Acc: 0.6692\n",
            "Epoch 15/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6767, Val Loss: 0.6755, Val Acc: 0.6481\n",
            "Epoch 16/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6726, Val Loss: 0.6769, Val Acc: 0.5882\n",
            "Epoch 17/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6694, Val Loss: 0.6666, Val Acc: 0.6549\n",
            "Epoch 18/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6640, Val Loss: 0.6734, Val Acc: 0.5781\n",
            "Epoch 19/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6615, Val Loss: 0.6796, Val Acc: 0.5173\n",
            "Epoch 20/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6589, Val Loss: 0.6597, Val Acc: 0.6734\n",
            "Epoch 21/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6522, Val Loss: 0.6487, Val Acc: 0.6700\n",
            "Epoch 22/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6462, Val Loss: 0.6544, Val Acc: 0.6473\n",
            "Epoch 23/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6359, Val Loss: 0.6548, Val Acc: 0.5443\n",
            "Epoch 24/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6357, Val Loss: 0.6327, Val Acc: 0.7089\n",
            "Epoch 25/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6320, Val Loss: 0.6286, Val Acc: 0.7105\n",
            "Epoch 26/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6212, Val Loss: 0.6222, Val Acc: 0.7257\n",
            "Epoch 27/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6167, Val Loss: 0.6286, Val Acc: 0.6819\n",
            "Epoch 28/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6109, Val Loss: 0.6103, Val Acc: 0.7266\n",
            "Epoch 29/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6025, Val Loss: 0.6044, Val Acc: 0.7198\n",
            "Epoch 30/30 - SimCLR_LoRA_Classifier - Train Loss: 0.6039, Val Loss: 0.6016, Val Acc: 0.7401\n",
            "Fold 5 - Modelo 'SimCLR_LoRA_Classifier' - Métricas Finais (Val): Acc: 0.740, Prec: 0.744, Rec: 0.740, F1: 0.739\n",
            "Modelo 'SimCLR_LoRA_Classifier' salvo para o Fold 5.\n",
            "\n",
            "--- Validação Cruzada Concluída ---\n",
            "\n",
            "Análise concluída.\n",
            "Scores F1 finais (do último fold processado com sucesso):\n",
            "  VibrationCNN: 0.7632\n",
            "  SimCLR_LoRA_Classifier: 0.7395\n",
            "Melhor modelo no último fold (baseado em F1): VibrationCNN (F1: 0.7632)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dGP--ZjLCw0R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}